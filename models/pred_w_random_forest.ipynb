{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the weekly rental price from 2023 to 2027 with Random forest Regression </br>\n",
    "In this notebook, we will train the random forest regressor model with the train dataset from 2013 to 2022, and predict the weekly rental price for each district for each year from 2023 - 2027 with this trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from matplotlib import pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory\n",
    "directory = \"random_forest_pred\"\n",
    "  \n",
    "# Parent Directory path\n",
    "parent_dir = \"../data/curated/\"\n",
    "\n",
    "# Path\n",
    "path = os.path.join(parent_dir, directory)\n",
    "\n",
    "# Create the directory\n",
    "os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['year', 'sa2_2021', 'residence_type', 'nbed', 'nbath', 'ncar',\n",
      "       'min_distance_to_cbd', 'min_distance_to_park', 'min_distance_to_prim',\n",
      "       'min_distance_to_second', 'min_distance_to_train',\n",
      "       'min_distance_to_hosp', 'min_distance_to_poli', 'min_distance_to_shop',\n",
      "       'weekly_rent', 'gdp(USD Millioins)', 'saving_rate(% of GDP)',\n",
      "       'income_per_person', 'population_density', 'crime_cases'],\n",
      "      dtype='object')\n",
      "        year     sa2_2021  residence_type  nbed  nbath  ncar  \\\n",
      "0       2013  204011057.0               1   2.0    1.0   0.0   \n",
      "1       2013  205051101.0               1   2.0    1.0   0.0   \n",
      "2       2013  204011057.0               1   2.0    1.0   0.0   \n",
      "3       2013  202011022.0               1   4.0    2.0   0.0   \n",
      "4       2013  208041195.0               0   1.0    1.0   0.0   \n",
      "...      ...          ...             ...   ...    ...   ...   \n",
      "172030  2022  205021086.0               1   3.0    1.0   1.0   \n",
      "172031  2022  217041479.0               1   3.0    2.0   2.0   \n",
      "172032  2022  208021177.0               1   2.0    2.0   1.0   \n",
      "172033  2022  206041506.0               0   1.0    1.0   1.0   \n",
      "172034  2022  206071139.0               0   1.0    1.0   1.0   \n",
      "\n",
      "        min_distance_to_cbd  min_distance_to_park  min_distance_to_prim  \\\n",
      "0                 227.97163              23.16035               7.35747   \n",
      "1                 223.66084               5.71742               6.50536   \n",
      "2                 243.25680               5.11222               0.20027   \n",
      "3                 140.35827              78.32509              10.66523   \n",
      "4                  13.86135               0.93250               1.32931   \n",
      "...                     ...                   ...                   ...   \n",
      "172030            293.28053               0.56012               1.21809   \n",
      "172031            258.29111               3.49087               5.08707   \n",
      "172032              9.47077               2.45011               1.33931   \n",
      "172033              1.84933               0.65199               1.10438   \n",
      "172034              4.58626               0.48042               0.49588   \n",
      "\n",
      "        min_distance_to_second  min_distance_to_train  min_distance_to_hosp  \\\n",
      "0                     16.96507               35.56825              21.35025   \n",
      "1                      6.76794                7.54355               7.42972   \n",
      "2                     36.72106               50.85341              36.63541   \n",
      "3                     11.91899               11.26906             177.44731   \n",
      "4                      3.49174                2.20800             177.44731   \n",
      "...                        ...                    ...                   ...   \n",
      "172030               114.77016               90.08591             140.56888   \n",
      "172031                 3.60570                8.37185               2.60312   \n",
      "172032                 1.62322                3.63291             140.56888   \n",
      "172033                 1.27940                1.87840             140.56888   \n",
      "172034                 1.47456                1.29233             140.56888   \n",
      "\n",
      "        min_distance_to_poli  min_distance_to_shop        gdp  saving_rate  \\\n",
      "0                   22.04660               9.35209  1536454.0     6.861393   \n",
      "1                    6.28177               9.35209  1536454.0     6.861393   \n",
      "2                    0.08478               9.35209  1536454.0     6.861393   \n",
      "3                   84.47341               9.35209  1536454.0     6.861393   \n",
      "4                   84.47341               3.96501  1536454.0     6.861393   \n",
      "...                      ...                   ...        ...          ...   \n",
      "172030              74.35608              13.64920  3305754.0    12.839000   \n",
      "172031              74.35608              13.64920  3305754.0    12.839000   \n",
      "172032              74.35608               1.97636  3305754.0    12.839000   \n",
      "172033              74.35608              13.64920  3305754.0    12.839000   \n",
      "172034               1.38884              13.64920  3305754.0    12.839000   \n",
      "\n",
      "        income_per_person  population_density  crime_cases  \n",
      "0            39683.563449            2.172408         86.0  \n",
      "1            47222.702327            5.425503         36.0  \n",
      "2            39683.563449            2.172408         86.0  \n",
      "3            43556.283562          473.765281       1288.0  \n",
      "4            86103.411528         2834.210526       1923.0  \n",
      "...                   ...                 ...          ...  \n",
      "172030       54365.266130          402.000000        281.0  \n",
      "172031       60828.473189          689.000000       3049.0  \n",
      "172032       98756.492866         3656.000000        759.0  \n",
      "172033       71305.473808         5791.000000       1788.0  \n",
      "172034       79065.119914         3954.000000       1401.0  \n",
      "\n",
      "[172018 rows x 19 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Tutorial 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "parent_dir_yr = \"../data/curated/2023_2027_data\"\n",
    "\n",
    "path = r'../data/curated/merged_dataset/'\n",
    "all_files = glob.glob(os.path.join(path , \"*.csv\"))\n",
    "\n",
    "li = []\n",
    "\n",
    "# To extract the same column names \n",
    "# Run the prediction dataset (2023-2027) first to extract its column names \n",
    "\n",
    "for filename in os.listdir(parent_dir_yr):\n",
    "    print(parent_dir_yr + \"/\" + filename)\n",
    "    merged_df_yr = spark.read.csv(parent_dir_yr + \"/\" + filename, header=True)\n",
    "\n",
    "    # Extract year from the file name \n",
    "    which_year = re.findall(r'\\d+', filename)\n",
    "\n",
    "    # Add year column to the dataset to fit the input into the model\n",
    "    merged_df_yr = merged_df_yr.withColumn(\"year\", lit(which_year[0]))\n",
    "    \n",
    "    merged_df_yr = merged_df_yr.toPandas()\n",
    "    sa2 = merged_df_yr[\"sa2_2021\"]\n",
    "    merged_df_yr = pd.get_dummies(data=merged_df_yr, columns=['sa2_2021'], prefix='sa2')\n",
    "    merged_df_yr = pd.get_dummies(data=merged_df_yr, columns=['residence_type'], prefix='resiType') \n",
    "\n",
    "    merged_df_yr.rename(columns = {'gdp(USD Millioins)':'gdp', 'saving_rate(% of GDP)':'saving_rate'}, inplace = True)\n",
    "\n",
    "    merged_df_yr.dropna(inplace=True)\n",
    "\n",
    "    # Get the dolumn names from prediction model after putting dummy variables on sa2 codes and residence type attributes\n",
    "    merged_df_yr_col = merged_df_yr.columns\n",
    "    \n",
    "# Run the tranining dataset (2013 - 2022)\n",
    "for filename in sorted(all_files):\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "# Merge the whole traninig dataset \n",
    "merged_df = pd.concat(li, axis=0, ignore_index=True)\n",
    "\n",
    "# Put sa2 codes and residence types into dummy variables as they are not treated as numerical in this model\n",
    "merged_df = pd.get_dummies(data=merged_df, columns=['sa2_2021'], prefix='sa2')\n",
    "merged_df = pd.get_dummies(data=merged_df, columns=['residence_type'], prefix='resiType') \n",
    "merged_df.drop(['address', 'latitude', 'longitude', 'postcode', 'sa2_2016'], axis=1, inplace=True)\n",
    "print(merged_df.columns)\n",
    "\n",
    "# Now, convert each column so that each has its approparite data type\n",
    "for col in merged_df.columns:\n",
    "    if 'resiType' in col:\n",
    "        merged_df[col] = merged_df[col].astype(int)\n",
    "    elif 'year' in col:\n",
    "        merged_df[col] = merged_df[col].astype(int)\n",
    "    elif 'sa2' in col:\n",
    "        merged_df[col] = merged_df[col].astype(int)\n",
    "    else:\n",
    "        merged_df[col] = merged_df[col].astype(float)\n",
    "\n",
    "# Rename the columns for better readability\n",
    "merged_df.rename({'gdp(USD Millioins)': 'gdp', 'saving_rate(% of GDP)': 'saving_rate'}, axis=1, inplace=True)\n",
    "merged_df.dropna(inplace=True)\n",
    "\n",
    "# Get our target class fro the merged training set\n",
    "y = merged_df['weekly_rent']\n",
    "merged_df.drop('weekly_rent', axis=1, inplace=True)\n",
    "\n",
    "# Only have columns that are common in the training and predicting dataset\n",
    "common_cols = list(set(merged_df).intersection(merged_df_yr_col))\n",
    "merged_df = merged_df[common_cols]\n",
    "\n",
    "X = merged_df\n",
    "print(X)\n",
    "\n",
    "# Now, run the random forest regressor on the merged tranining dataset\n",
    "sel = RandomForestRegressor(n_estimators = 100, random_state=42)\n",
    "sel.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to dummy variable function due to a nominal attribute, sa2 code, the column names were not constant between the traninig and prediction dataset. (There are a few of suburbs missing from a training dataset, while the prediction dataset has all suburbs.) This is not accepted in random forest regression where they require to have the exact same features to train and predict. Therefore, the suburb sa2 codes that the trained model didn't see from the training dataset have been dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 02:19:32 WARN Utils: Your hostname, Hyunjin-Win11 resolves to a loopback address: 127.0.1.1; using 192.168.245.16 instead (on interface eth0)\n",
      "22/10/05 02:19:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hyunjinp/generic-real-estate-consulting-project-group-43/models/All_atonce_random_forest.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/hyunjinp/generic-real-estate-consulting-project-group-43/models/All_atonce_random_forest.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctions\u001b[39;00m \u001b[39mimport\u001b[39;00m lit\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/hyunjinp/generic-real-estate-consulting-project-group-43/models/All_atonce_random_forest.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Create a spark session (which will run spark jobs)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/hyunjinp/generic-real-estate-consulting-project-group-43/models/All_atonce_random_forest.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m spark \u001b[39m=\u001b[39m (\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/hyunjinp/generic-real-estate-consulting-project-group-43/models/All_atonce_random_forest.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     SparkSession\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mappName(\u001b[39m\"\u001b[39;49m\u001b[39mMAST30034 Tutorial 1\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/hyunjinp/generic-real-estate-consulting-project-group-43/models/All_atonce_random_forest.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.sql.repl.eagerEval.enabled\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m) \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/hyunjinp/generic-real-estate-consulting-project-group-43/models/All_atonce_random_forest.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.sql.parquet.cacheMetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtrue\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/hyunjinp/generic-real-estate-consulting-project-group-43/models/All_atonce_random_forest.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.sql.session.timeZone\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mEtc/UTC\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/hyunjinp/generic-real-estate-consulting-project-group-43/models/All_atonce_random_forest.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m.\u001b[39;49mgetOrCreate()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/hyunjinp/generic-real-estate-consulting-project-group-43/models/All_atonce_random_forest.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/hyunjinp/generic-real-estate-consulting-project-group-43/models/All_atonce_random_forest.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Check the accuracy before predicting with the trained random forest regressor\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/hyunjinp/generic-real-estate-consulting-project-group-43/models/All_atonce_random_forest.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Predict for the next 5 years\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/hyunjinp/generic-real-estate-consulting-project-group-43/models/All_atonce_random_forest.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m parent_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../data/curated/2023_2027_data\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py:269\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m     sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[1;32m    268\u001b[0m \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[1;32m    270\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/context.py:483\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    482\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 483\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[1;32m    484\u001b[0m     \u001b[39massert\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/context.py:195\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    191\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     )\n\u001b[0;32m--> 195\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[1;32m    196\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[1;32m    198\u001b[0m         master,\n\u001b[1;32m    199\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m         udf_profiler_cls,\n\u001b[1;32m    209\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/context.py:417\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    416\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[0;32m--> 417\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[1;32m    418\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[1;32m    420\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/java_gateway.py:103\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m proc\u001b[39m.\u001b[39mpoll() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 103\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.1\u001b[39;49m)\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m    106\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mJava gateway process exited before sending its port number\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Now, Predict for the next 5 years\n",
    "for filename in os.listdir(parent_dir_yr):\n",
    "    merged_df_yr = spark.read.csv(parent_dir_yr + \"/\" + filename, header=True)\n",
    "\n",
    "    # Extract year from the file name \n",
    "    which_year = re.findall(r'\\d+', filename)\n",
    "\n",
    "    # Add year column to the dataset to fit the input into the model\n",
    "    merged_df_yr = merged_df_yr.withColumn(\"year\", lit(which_year[0]))\n",
    "    \n",
    "    merged_df_yr = merged_df_yr.toPandas()\n",
    "    \n",
    "    merged_df_yr = pd.get_dummies(data=merged_df_yr, columns=['sa2_2021'], prefix='sa2')\n",
    "    merged_df_yr = pd.get_dummies(data=merged_df_yr, columns=['residence_type'], prefix='resiType') \n",
    "    merged_df_yr\n",
    "\n",
    "    for col in merged_df_yr.columns:\n",
    "        if 'resiType' in col:\n",
    "            merged_df_yr[col] = merged_df_yr[col].astype(int)\n",
    "        elif 'year' in col:\n",
    "            merged_df_yr[col] = merged_df_yr[col].astype(int)\n",
    "        elif 'sa2' in col:\n",
    "            merged_df_yr[col] = merged_df_yr[col].astype(int)\n",
    "        else:\n",
    "            merged_df_yr[col] = merged_df_yr[col].astype(float)\n",
    "\n",
    "    merged_df_yr.rename(columns = {'gdp(USD Millioins)':'gdp', 'saving_rate(% of GDP)':'saving_rate'}, inplace = True)\n",
    "\n",
    "    merged_df_yr.dropna(inplace=True)\n",
    "\n",
    "    # Reorder the columns with only common columns between the training and prediction dataset\n",
    "    merged_df_yr = merged_df_yr[common_cols]\n",
    "\n",
    "    print(merged_df_yr)\n",
    "\n",
    "    # Predict the weekly rental price with random forest tree\n",
    "    prediction = sel.predict(merged_df_yr)\n",
    "\n",
    "    # Reverse the dummy variable of SA2 codes (eg. column 'sa2_20211113' -> '20211113' in sa2_2021 column)\n",
    "    sa2_list = []\n",
    "    for i in common_cols:\n",
    "        if \"sa2\" in i:\n",
    "            sa2_list.append(int(i[4:]))\n",
    "\n",
    "    sa2_list.sort()\n",
    "\n",
    "    new_columns_sa2 = []\n",
    "    for i in sa2:\n",
    "        i = int(i)\n",
    "        if i in sa2_list:\n",
    "            new_columns_sa2.append(i)\n",
    "\n",
    "    # Now, put the predictions into the csv files \n",
    "    new_csv_name = \"../Desktop/random_forest_pred/\" + filename\n",
    "\n",
    "    data = {'year': merged_df_yr['year'],\n",
    "            'sa2_2021': new_columns_sa2,\n",
    "            'predicted_price': prediction }\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df)\n",
    "    df.to_csv(new_csv_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are completed with the random forest regressor. The key findings will be illustrated in `growthRateCalc.ipynb` notebook. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
