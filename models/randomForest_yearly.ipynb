{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new folder for random forest preiction for rental price\n",
    "import os\n",
    "  \n",
    "# Directory\n",
    "directory = \"random_forest_pred\"\n",
    "  \n",
    "# Parent Directory path\n",
    "parent_dir = \"../data/curated/\"\n",
    "\n",
    "# Path\n",
    "path = os.path.join(parent_dir, directory)\n",
    "\n",
    "# Create the directory\n",
    "os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2018_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2014_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2017_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2015_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2019_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2016_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Predict for the next 5 years\n",
    "import os\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import FloatType\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Tutorial 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "parent_dir = \"../data/curated/merged_dataset\"\n",
    "\n",
    "sel_pred = RandomForestRegressor(n_estimators = 100)\n",
    "\n",
    "for filename in os.listdir(parent_dir):\n",
    "    print(filename, \"----------------------------------------------------------------------------------------------------------------\")\n",
    "    merged_df_yr = spark.read.csv(parent_dir + \"/\" + filename, header=True)\n",
    "\n",
    "    merged_df_yr = merged_df_yr.drop(\"address\",\"latitude\",\"longitude\",\"postcode\",\"sa2_2016\")\n",
    "    \n",
    "    for c in merged_df_yr.columns:\n",
    "        if (c not in  ['address', 'residence_type']):\n",
    "            merged_df_yr = merged_df_yr.withColumn(c,merged_df_yr[c].cast(FloatType())) \n",
    "\n",
    "    merged_df_yr = merged_df_yr.toPandas()\n",
    "\n",
    "    merged_df_yr['residence_type'] = merged_df_yr['residence_type'].astype('category')\n",
    "    merged_df_yr['residence_type'] = merged_df_yr['residence_type'].cat.codes\n",
    "\n",
    "    merged_df_yr.iloc[:, 13:21] = merged_df_yr.iloc[:, 13:21].replace(np.nan, 99999)\n",
    "\n",
    "    merged_df_yr.rename(columns = {'gdp(USD Millioins)':'gdp', 'saving_rate(% of GDP)':'saving_rate'}, inplace = True)\n",
    "    \n",
    "    merged_df_yr = merged_df_yr.dropna()\n",
    "\n",
    "    \n",
    "    X = merged_df_yr.drop(['weekly_rent'], axis=1)\n",
    "    y = np.log(merged_df_yr['weekly_rent'])\n",
    "    # Train with the whole dataset for the actual prediction for the next 5 years (This training is not for feature enginnering & accuracy test)\n",
    "    sel_pred.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Predict for the next 5 years\n",
    "import os\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import FloatType\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Tutorial 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "parent_dir = \"../data/curated/2023_2027_data\"\n",
    "\n",
    "for filename in os.listdir(parent_dir):\n",
    "    merged_df_yr = spark.read.csv(parent_dir + \"/\" + filename, header=True)\n",
    "\n",
    "    # Extract year from the file name \n",
    "    which_year = re.findall(r'\\d+', filename)\n",
    "\n",
    "    # Add year column to the dataset to fit the input into the model\n",
    "    merged_df_yr = merged_df_yr.withColumn(\"year\", lit(which_year[0]))\n",
    "\n",
    "    for c in merged_df_yr.columns:\n",
    "        merged_df_yr = merged_df_yr.withColumn(c,merged_df_yr[c].cast(FloatType())) \n",
    "\n",
    "    merged_df_yr = merged_df_yr.toPandas()\n",
    "    merged_df_yr.rename(columns = {'gdp(USD Millioins)':'gdp', 'saving_rate(% of GDP)':'saving_rate'}, inplace = True)\n",
    "    merged_df_yr['residence_type'] = merged_df_yr['residence_type'].astype('category')\n",
    "    merged_df_yr['residence_type'] = merged_df_yr['residence_type'].cat.codes\n",
    "    merged_df_yr.dropna(inplace=True)\n",
    "\n",
    "    # Reorder the columns \n",
    "    merged_df_yr_reordered = merged_df_yr[['year', 'sa2_2021', 'residence_type', 'nbed', 'nbath', 'ncar',\n",
    "       'min_distance_to_cbd', 'min_distance_to_park', 'min_distance_to_prim',\n",
    "       'min_distance_to_second', 'min_distance_to_train',\n",
    "       'min_distance_to_hosp', 'min_distance_to_poli', 'min_distance_to_shop',\n",
    "       'gdp', 'saving_rate', 'income_per_person', 'population_density',\n",
    "       'crime_cases']]\n",
    "\n",
    "    # Predict with random forest tree\n",
    "    prediction = sel_pred.predict(merged_df_yr_reordered)\n",
    "\n",
    "    new_csv_name = \"../data/curated/random_forest_pred/\" + filename\n",
    "\n",
    "    np.savetxt(new_csv_name, prediction, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
