{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import IntegerType\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import lit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new folder for random forest preiction for rental price\n",
    "# Directory\n",
    "directory = \"random_forest_pred\"\n",
    "  \n",
    "# Parent Directory path\n",
    "parent_dir = \"../data/curated/\"\n",
    "\n",
    "# Path\n",
    "path = os.path.join(parent_dir, directory)\n",
    "\n",
    "# Create the directory\n",
    "os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 23:18:14 WARN Utils: Your hostname, Hyunjin-Win11 resolves to a loopback address: 127.0.1.1; using 192.168.245.16 instead (on interface eth0)\n",
      "22/10/04 23:18:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 23:18:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Tutorial 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "parent_dir = \"../data/curated/merged_dataset\"\n",
    "\n",
    "\n",
    "sel = RandomForestRegressor(n_estimators = 100)\n",
    "\n",
    "for filename in os.listdir(parent_dir):\n",
    "    if filename != \"2022_merged_data.csv\":\n",
    "        merged_df_yr = spark.read.csv(parent_dir + \"/\" + filename, header=True)\n",
    "\n",
    "        merged_df_yr = merged_df_yr.drop(\"address\",\"latitude\",\"longitude\",\"postcode\",\"sa2_2016\")\n",
    "        \n",
    "        for c in merged_df_yr.columns:\n",
    "            if (c not in  ['address', 'residence_type']):\n",
    "                merged_df_yr = merged_df_yr.withColumn(c,merged_df_yr[c].cast(FloatType())) \n",
    "\n",
    "        merged_df_yr = merged_df_yr.toPandas()\n",
    "\n",
    "        merged_df_yr['residence_type'] = merged_df_yr['residence_type'].astype('category')\n",
    "        merged_df_yr['residence_type'] = merged_df_yr['residence_type'].cat.codes\n",
    "\n",
    "        merged_df_yr.iloc[:, 13:21] = merged_df_yr.iloc[:, 13:21].replace(np.nan, 99999)\n",
    "\n",
    "        merged_df_yr.rename(columns = {'gdp(USD Millioins)':'gdp', 'saving_rate(% of GDP)':'saving_rate'}, inplace = True)\n",
    "        \n",
    "        merged_df_yr = merged_df_yr.dropna()\n",
    "   \n",
    "        X = merged_df_yr.drop(['weekly_rent'], axis=1)\n",
    "        y = np.log(merged_df_yr['weekly_rent'])\n",
    "        # Train with the whole dataset for the actual prediction for the next 5 years (This training is not for feature enginnering & accuracy test)\n",
    "        sel.fit(X, y)\n",
    "\n",
    "accuracy_input = spark.read.csv(parent_dir + \"/\" + \"2022_merged_data.csv\", header=True)\n",
    "accuracy_input = accuracy_input.drop(\"address\",\"latitude\",\"longitude\",\"postcode\",\"sa2_2016\")\n",
    "        \n",
    "for c in accuracy_input.columns:\n",
    "    if (c not in  ['address', 'residence_type']):\n",
    "        accuracy_input = accuracy_input.withColumn(c,accuracy_input[c].cast(FloatType())) \n",
    "\n",
    "accuracy_input = accuracy_input.toPandas()\n",
    "\n",
    "accuracy_input['residence_type'] = accuracy_input['residence_type'].astype('category')\n",
    "accuracy_input['residence_type'] = accuracy_input['residence_type'].cat.codes\n",
    "\n",
    "accuracy_input.iloc[:, 13:21] = accuracy_input.iloc[:, 13:21].replace(np.nan, 99999)\n",
    "\n",
    "accuracy_input.rename(columns = {'gdp(USD Millioins)':'gdp', 'saving_rate(% of GDP)':'saving_rate'}, inplace = True)\n",
    "        \n",
    "accuracy_input = accuracy_input.dropna()\n",
    "   \n",
    "X_accuracy = accuracy_input.drop(['weekly_rent'], axis=1)\n",
    "y_accuracy_true = np.log(accuracy_input['weekly_rent'])\n",
    "\n",
    "prediction = sel.predict(X_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41158.655941285\n",
      "202.8759619602209\n",
      "Mean Absolute Error: 103.06 degrees.\n",
      "Accuracy: 83.02 %.\n"
     ]
    }
   ],
   "source": [
    "# Check the accuracy before predicting with the trained random forest regressor\n",
    "y_accuracy_true = np.exp(y_accuracy_true)\n",
    "prediction = np.exp(prediction)\n",
    "mse = mean_squared_error(y_accuracy_true, prediction)\n",
    "rmse = mse**.5\n",
    "print(mse)\n",
    "print(rmse)\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = np.abs(prediction - y_accuracy_true)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', np.round(np.mean(errors), 2), 'degrees.')\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / y_accuracy_true)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', np.round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2021_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2020_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2022_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2018_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2014_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2017_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2015_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2019_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2016_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Tutorial 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "parent_dir = \"../data/curated/merged_dataset\"\n",
    "\n",
    "sel_pred = RandomForestRegressor(n_estimators = 100)\n",
    "\n",
    "for filename in os.listdir(parent_dir):\n",
    "    print(filename, \"----------------------------------------------------------------------------------------------------------------\")\n",
    "    merged_df_yr = spark.read.csv(parent_dir + \"/\" + filename, header=True)\n",
    "\n",
    "    merged_df_yr = merged_df_yr.drop(\"address\",\"latitude\",\"longitude\",\"postcode\",\"sa2_2016\")\n",
    "    \n",
    "    for c in merged_df_yr.columns:\n",
    "        if (c not in  ['address', 'residence_type']):\n",
    "            merged_df_yr = merged_df_yr.withColumn(c,merged_df_yr[c].cast(FloatType())) \n",
    "\n",
    "    merged_df_yr = merged_df_yr.toPandas()\n",
    "\n",
    "    merged_df_yr['residence_type'] = merged_df_yr['residence_type'].astype('category')\n",
    "    merged_df_yr['residence_type'] = merged_df_yr['residence_type'].cat.codes\n",
    "\n",
    "    merged_df_yr.iloc[:, 13:21] = merged_df_yr.iloc[:, 13:21].replace(np.nan, 99999)\n",
    "\n",
    "    merged_df_yr.rename(columns = {'gdp(USD Millioins)':'gdp', 'saving_rate(% of GDP)':'saving_rate'}, inplace = True)\n",
    "    \n",
    "    merged_df_yr = merged_df_yr.dropna()\n",
    "\n",
    "    \n",
    "    X = merged_df_yr.drop(['weekly_rent'], axis=1)\n",
    "    y = np.log(merged_df_yr['weekly_rent'])\n",
    "    # Train with the whole dataset for the actual prediction for the next 5 years (This training is not for feature enginnering & accuracy test)\n",
    "    sel_pred.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10414 10414 10414\n",
      "       year   sa2_2021  weekly_rent\n",
      "0      2023  201011008   169.240076\n",
      "1      2023  201011008   247.650383\n",
      "2      2023  201011008   251.456622\n",
      "3      2023  201011008   177.500942\n",
      "4      2023  201011008   290.133341\n",
      "...     ...        ...          ...\n",
      "10409  2023  217041472   314.601953\n",
      "10410  2023  217041472   330.345853\n",
      "10411  2023  217041472   387.327074\n",
      "10412  2023  217041472   385.634188\n",
      "10413  2023  217041472   381.143056\n",
      "\n",
      "[10414 rows x 3 columns]\n",
      "10414 10414 10414\n",
      "       year   sa2_2021  weekly_rent\n",
      "0      2024  201011008   169.328021\n",
      "1      2024  201011008   247.650383\n",
      "2      2024  201011008   251.456622\n",
      "3      2024  201011008   177.500942\n",
      "4      2024  201011008   290.057330\n",
      "...     ...        ...          ...\n",
      "10409  2024  217041472   316.010803\n",
      "10410  2024  217041472   332.676005\n",
      "10411  2024  217041472   388.761856\n",
      "10412  2024  217041472   389.084515\n",
      "10413  2024  217041472   382.554930\n",
      "\n",
      "[10414 rows x 3 columns]\n",
      "10414 10414 10414\n",
      "       year   sa2_2021  weekly_rent\n",
      "0      2027  201011008   170.155974\n",
      "1      2027  201011008   245.980794\n",
      "2      2027  201011008   249.863640\n",
      "3      2027  201011008   178.634400\n",
      "4      2027  201011008   292.303476\n",
      "...     ...        ...          ...\n",
      "10409  2027  217041472   314.929162\n",
      "10410  2027  217041472   334.765713\n",
      "10411  2027  217041472   393.937410\n",
      "10412  2027  217041472   408.013434\n",
      "10413  2027  217041472   391.635305\n",
      "\n",
      "[10414 rows x 3 columns]\n",
      "10414 10414 10414\n",
      "       year   sa2_2021  weekly_rent\n",
      "0      2026  201011008   169.328021\n",
      "1      2026  201011008   247.650383\n",
      "2      2026  201011008   251.456622\n",
      "3      2026  201011008   177.500942\n",
      "4      2026  201011008   290.057330\n",
      "...     ...        ...          ...\n",
      "10409  2026  217041472   316.010803\n",
      "10410  2026  217041472   336.842836\n",
      "10411  2026  217041472   402.191851\n",
      "10412  2026  217041472   400.901887\n",
      "10413  2026  217041472   395.088601\n",
      "\n",
      "[10414 rows x 3 columns]\n",
      "10414 10414 10414\n",
      "       year   sa2_2021  weekly_rent\n",
      "0      2025  201011008   169.328021\n",
      "1      2025  201011008   247.650383\n",
      "2      2025  201011008   251.456622\n",
      "3      2025  201011008   177.500942\n",
      "4      2025  201011008   290.057330\n",
      "...     ...        ...          ...\n",
      "10409  2025  217041472   316.010803\n",
      "10410  2025  217041472   332.458565\n",
      "10411  2025  217041472   388.977951\n",
      "10412  2025  217041472   389.084515\n",
      "10413  2025  217041472   382.304888\n",
      "\n",
      "[10414 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Predict for the next 5 years\n",
    "parent_dir = \"../data/curated/2023_2027_data\"\n",
    "\n",
    "for filename in os.listdir(parent_dir):\n",
    "    merged_df_yr = spark.read.csv(parent_dir + \"/\" + filename, header=True)\n",
    "\n",
    "    # Extract year from the file name \n",
    "    which_year = re.findall(r'\\d+', filename)\n",
    "\n",
    "    # Add year column to the dataset to fit the input into the model\n",
    "    merged_df_yr = merged_df_yr.withColumn(\"year\", lit(which_year[0]))\n",
    "\n",
    "    for c in merged_df_yr.columns:\n",
    "        merged_df_yr = merged_df_yr.withColumn(c,merged_df_yr[c].cast(FloatType())) \n",
    "\n",
    "    merged_df_yr = merged_df_yr.toPandas()\n",
    "    merged_df_yr.rename(columns = {'gdp(USD Millioins)':'gdp', 'saving_rate(% of GDP)':'saving_rate'}, inplace = True)\n",
    "    merged_df_yr['residence_type'] = merged_df_yr['residence_type'].astype('category')\n",
    "    merged_df_yr['residence_type'] = merged_df_yr['residence_type'].cat.codes\n",
    "    merged_df_yr.dropna(inplace=True)\n",
    "\n",
    "    # Reorder the columns \n",
    "    merged_df_yr_reordered = merged_df_yr[['year', 'sa2_2021', 'residence_type', 'nbed', 'nbath', 'ncar',\n",
    "       'min_distance_to_cbd', 'min_distance_to_park', 'min_distance_to_prim',\n",
    "       'min_distance_to_second', 'min_distance_to_train',\n",
    "       'min_distance_to_hosp', 'min_distance_to_poli', 'min_distance_to_shop',\n",
    "       'gdp', 'saving_rate', 'income_per_person', 'population_density',\n",
    "       'crime_cases']]\n",
    "\n",
    "    # Predict with random forest tree\n",
    "    prediction = sel_pred.predict(merged_df_yr_reordered)\n",
    "    prediction = np.exp(prediction)\n",
    "    new_csv_name = \"../data/curated/random_forest_pred/\" + filename\n",
    "    print(len(merged_df_yr_reordered['year'].astype(int)), len(merged_df_yr_reordered['sa2_2021'].astype(int)), len(prediction))\n",
    "    data = {'year': merged_df_yr_reordered['year'].astype(int),\n",
    "            'sa2_2021': merged_df_yr_reordered['sa2_2021'].astype(int),\n",
    "            'weekly_rent': prediction }\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df)\n",
    "    df.to_csv(new_csv_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
