{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import IntegerType\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import lit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new folder for random forest preiction for rental price\n",
    "# Directory\n",
    "directory = \"random_forest_pred\"\n",
    "  \n",
    "# Parent Directory path\n",
    "parent_dir = \"../data/curated/\"\n",
    "\n",
    "# Path\n",
    "path = os.path.join(parent_dir, directory)\n",
    "\n",
    "# Create the directory\n",
    "os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 23:18:14 WARN Utils: Your hostname, Hyunjin-Win11 resolves to a loopback address: 127.0.1.1; using 192.168.245.16 instead (on interface eth0)\n",
      "22/10/04 23:18:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 23:18:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Tutorial 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "parent_dir = \"../data/curated/merged_dataset\"\n",
    "\n",
    "\n",
    "sel = RandomForestRegressor(n_estimators = 100)\n",
    "\n",
    "for filename in os.listdir(parent_dir):\n",
    "    if filename != \"2022_merged_data.csv\":\n",
    "        merged_df_yr = spark.read.csv(parent_dir + \"/\" + filename, header=True)\n",
    "\n",
    "        merged_df_yr = merged_df_yr.drop(\"address\",\"latitude\",\"longitude\",\"postcode\",\"sa2_2016\")\n",
    "        \n",
    "        for c in merged_df_yr.columns:\n",
    "            if (c not in  ['address', 'residence_type']):\n",
    "                merged_df_yr = merged_df_yr.withColumn(c,merged_df_yr[c].cast(FloatType())) \n",
    "\n",
    "        merged_df_yr = merged_df_yr.toPandas()\n",
    "\n",
    "        merged_df_yr['residence_type'] = merged_df_yr['residence_type'].astype('category')\n",
    "        merged_df_yr['residence_type'] = merged_df_yr['residence_type'].cat.codes\n",
    "\n",
    "        merged_df_yr.iloc[:, 13:21] = merged_df_yr.iloc[:, 13:21].replace(np.nan, 99999)\n",
    "\n",
    "        merged_df_yr.rename(columns = {'gdp(USD Millioins)':'gdp', 'saving_rate(% of GDP)':'saving_rate'}, inplace = True)\n",
    "        \n",
    "        merged_df_yr = merged_df_yr.dropna()\n",
    "   \n",
    "        X = merged_df_yr.drop(['weekly_rent'], axis=1)\n",
    "        y = np.log(merged_df_yr['weekly_rent'])\n",
    "        # Train with the whole dataset for the actual prediction for the next 5 years (This training is not for feature enginnering & accuracy test)\n",
    "        sel.fit(X, y)\n",
    "\n",
    "accuracy_input = spark.read.csv(parent_dir + \"/\" + \"2022_merged_data.csv\", header=True)\n",
    "accuracy_input = accuracy_input.drop(\"address\",\"latitude\",\"longitude\",\"postcode\",\"sa2_2016\")\n",
    "        \n",
    "for c in accuracy_input.columns:\n",
    "    if (c not in  ['address', 'residence_type']):\n",
    "        accuracy_input = accuracy_input.withColumn(c,accuracy_input[c].cast(FloatType())) \n",
    "\n",
    "accuracy_input = accuracy_input.toPandas()\n",
    "\n",
    "accuracy_input['residence_type'] = accuracy_input['residence_type'].astype('category')\n",
    "accuracy_input['residence_type'] = accuracy_input['residence_type'].cat.codes\n",
    "\n",
    "accuracy_input.iloc[:, 13:21] = accuracy_input.iloc[:, 13:21].replace(np.nan, 99999)\n",
    "\n",
    "accuracy_input.rename(columns = {'gdp(USD Millioins)':'gdp', 'saving_rate(% of GDP)':'saving_rate'}, inplace = True)\n",
    "        \n",
    "accuracy_input = accuracy_input.dropna()\n",
    "   \n",
    "X_accuracy = accuracy_input.drop(['weekly_rent'], axis=1)\n",
    "y_accuracy_true = np.log(accuracy_input['weekly_rent'])\n",
    "\n",
    "prediction = sel.predict(X_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41158.655941285\n",
      "202.8759619602209\n",
      "Mean Absolute Error: 103.06 degrees.\n",
      "Accuracy: 83.02 %.\n"
     ]
    }
   ],
   "source": [
    "# Check the accuracy before predicting with the trained random forest regressor\n",
    "y_accuracy_true = np.exp(y_accuracy_true)\n",
    "prediction = np.exp(prediction)\n",
    "mse = mean_squared_error(y_accuracy_true, prediction)\n",
    "rmse = mse**.5\n",
    "print(mse)\n",
    "print(rmse)\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = np.abs(prediction - y_accuracy_true)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', np.round(np.mean(errors), 2), 'degrees.')\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / y_accuracy_true)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', np.round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2021_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2020_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2022_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2018_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2014_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2017_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2015_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2019_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2016_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Tutorial 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "parent_dir = \"../data/curated/merged_dataset\"\n",
    "\n",
    "sel_pred = RandomForestRegressor(n_estimators = 100)\n",
    "\n",
    "for filename in os.listdir(parent_dir):\n",
    "    print(filename, \"----------------------------------------------------------------------------------------------------------------\")\n",
    "    merged_df_yr = spark.read.csv(parent_dir + \"/\" + filename, header=True)\n",
    "\n",
    "    merged_df_yr = merged_df_yr.drop(\"address\",\"latitude\",\"longitude\",\"postcode\",\"sa2_2016\")\n",
    "    \n",
    "    for c in merged_df_yr.columns:\n",
    "        if (c not in  ['address', 'residence_type']):\n",
    "            merged_df_yr = merged_df_yr.withColumn(c,merged_df_yr[c].cast(FloatType())) \n",
    "\n",
    "    merged_df_yr = merged_df_yr.toPandas()\n",
    "\n",
    "    merged_df_yr['residence_type'] = merged_df_yr['residence_type'].astype('category')\n",
    "    merged_df_yr['residence_type'] = merged_df_yr['residence_type'].cat.codes\n",
    "\n",
    "    merged_df_yr.iloc[:, 13:21] = merged_df_yr.iloc[:, 13:21].replace(np.nan, 99999)\n",
    "\n",
    "    merged_df_yr.rename(columns = {'gdp(USD Millioins)':'gdp', 'saving_rate(% of GDP)':'saving_rate'}, inplace = True)\n",
    "    \n",
    "    merged_df_yr = merged_df_yr.dropna()\n",
    "\n",
    "    \n",
    "    X = merged_df_yr.drop(['weekly_rent'], axis=1)\n",
    "    y = np.log(merged_df_yr['weekly_rent'])\n",
    "    # Train with the whole dataset for the actual prediction for the next 5 years (This training is not for feature enginnering & accuracy test)\n",
    "    sel_pred.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        201011001\n",
      "1        201011001\n",
      "2        201011001\n",
      "3        201011001\n",
      "4        201011001\n",
      "           ...    \n",
      "10409    217041480\n",
      "10410    217041480\n",
      "10411    217041480\n",
      "10412    217041480\n",
      "10413    217041480\n",
      "Name: sa2_2021, Length: 10414, dtype: int32\n",
      "0        201011001\n",
      "1        201011001\n",
      "2        201011001\n",
      "3        201011001\n",
      "4        201011001\n",
      "           ...    \n",
      "10409    217041480\n",
      "10410    217041480\n",
      "10411    217041480\n",
      "10412    217041480\n",
      "10413    217041480\n",
      "Name: sa2_2021, Length: 10414, dtype: int64\n",
      "       year   sa2_2021  predicted_price\n",
      "0      2023  201011001       169.240076\n",
      "1      2023  201011001       247.650383\n",
      "2      2023  201011001       251.456622\n",
      "3      2023  201011001       177.500942\n",
      "4      2023  201011001       290.133341\n",
      "...     ...        ...              ...\n",
      "10409  2023  217041480       314.601953\n",
      "10410  2023  217041480       330.345853\n",
      "10411  2023  217041480       387.327074\n",
      "10412  2023  217041480       385.634188\n",
      "10413  2023  217041480       381.143056\n",
      "\n",
      "[10414 rows x 3 columns]\n",
      "0        201011001\n",
      "1        201011001\n",
      "2        201011001\n",
      "3        201011001\n",
      "4        201011001\n",
      "           ...    \n",
      "10409    217041480\n",
      "10410    217041480\n",
      "10411    217041480\n",
      "10412    217041480\n",
      "10413    217041480\n",
      "Name: sa2_2021, Length: 10414, dtype: int32\n",
      "0        201011001\n",
      "1        201011001\n",
      "2        201011001\n",
      "3        201011001\n",
      "4        201011001\n",
      "           ...    \n",
      "10409    217041480\n",
      "10410    217041480\n",
      "10411    217041480\n",
      "10412    217041480\n",
      "10413    217041480\n",
      "Name: sa2_2021, Length: 10414, dtype: int64\n",
      "       year   sa2_2021  predicted_price\n",
      "0      2024  201011001       169.328021\n",
      "1      2024  201011001       247.650383\n",
      "2      2024  201011001       251.456622\n",
      "3      2024  201011001       177.500942\n",
      "4      2024  201011001       290.057330\n",
      "...     ...        ...              ...\n",
      "10409  2024  217041480       316.010803\n",
      "10410  2024  217041480       332.676005\n",
      "10411  2024  217041480       388.761856\n",
      "10412  2024  217041480       389.084515\n",
      "10413  2024  217041480       382.554930\n",
      "\n",
      "[10414 rows x 3 columns]\n",
      "0        201011001\n",
      "1        201011001\n",
      "2        201011001\n",
      "3        201011001\n",
      "4        201011001\n",
      "           ...    \n",
      "10409    217041480\n",
      "10410    217041480\n",
      "10411    217041480\n",
      "10412    217041480\n",
      "10413    217041480\n",
      "Name: sa2_2021, Length: 10414, dtype: int32\n",
      "0        201011001\n",
      "1        201011001\n",
      "2        201011001\n",
      "3        201011001\n",
      "4        201011001\n",
      "           ...    \n",
      "10409    217041480\n",
      "10410    217041480\n",
      "10411    217041480\n",
      "10412    217041480\n",
      "10413    217041480\n",
      "Name: sa2_2021, Length: 10414, dtype: int64\n",
      "       year   sa2_2021  predicted_price\n",
      "0      2027  201011001       170.155974\n",
      "1      2027  201011001       245.980794\n",
      "2      2027  201011001       249.863640\n",
      "3      2027  201011001       178.634400\n",
      "4      2027  201011001       292.303476\n",
      "...     ...        ...              ...\n",
      "10409  2027  217041480       314.929162\n",
      "10410  2027  217041480       334.765713\n",
      "10411  2027  217041480       393.937410\n",
      "10412  2027  217041480       408.013434\n",
      "10413  2027  217041480       391.635305\n",
      "\n",
      "[10414 rows x 3 columns]\n",
      "0        201011001\n",
      "1        201011001\n",
      "2        201011001\n",
      "3        201011001\n",
      "4        201011001\n",
      "           ...    \n",
      "10409    217041480\n",
      "10410    217041480\n",
      "10411    217041480\n",
      "10412    217041480\n",
      "10413    217041480\n",
      "Name: sa2_2021, Length: 10414, dtype: int32\n",
      "0        201011001\n",
      "1        201011001\n",
      "2        201011001\n",
      "3        201011001\n",
      "4        201011001\n",
      "           ...    \n",
      "10409    217041480\n",
      "10410    217041480\n",
      "10411    217041480\n",
      "10412    217041480\n",
      "10413    217041480\n",
      "Name: sa2_2021, Length: 10414, dtype: int64\n",
      "       year   sa2_2021  predicted_price\n",
      "0      2026  201011001       169.328021\n",
      "1      2026  201011001       247.650383\n",
      "2      2026  201011001       251.456622\n",
      "3      2026  201011001       177.500942\n",
      "4      2026  201011001       290.057330\n",
      "...     ...        ...              ...\n",
      "10409  2026  217041480       316.010803\n",
      "10410  2026  217041480       336.842836\n",
      "10411  2026  217041480       402.191851\n",
      "10412  2026  217041480       400.901887\n",
      "10413  2026  217041480       395.088601\n",
      "\n",
      "[10414 rows x 3 columns]\n",
      "0        201011001\n",
      "1        201011001\n",
      "2        201011001\n",
      "3        201011001\n",
      "4        201011001\n",
      "           ...    \n",
      "10409    217041480\n",
      "10410    217041480\n",
      "10411    217041480\n",
      "10412    217041480\n",
      "10413    217041480\n",
      "Name: sa2_2021, Length: 10414, dtype: int32\n",
      "0        201011001\n",
      "1        201011001\n",
      "2        201011001\n",
      "3        201011001\n",
      "4        201011001\n",
      "           ...    \n",
      "10409    217041480\n",
      "10410    217041480\n",
      "10411    217041480\n",
      "10412    217041480\n",
      "10413    217041480\n",
      "Name: sa2_2021, Length: 10414, dtype: int64\n",
      "       year   sa2_2021  predicted_price\n",
      "0      2025  201011001       169.328021\n",
      "1      2025  201011001       247.650383\n",
      "2      2025  201011001       251.456622\n",
      "3      2025  201011001       177.500942\n",
      "4      2025  201011001       290.057330\n",
      "...     ...        ...              ...\n",
      "10409  2025  217041480       316.010803\n",
      "10410  2025  217041480       332.458565\n",
      "10411  2025  217041480       388.977951\n",
      "10412  2025  217041480       389.084515\n",
      "10413  2025  217041480       382.304888\n",
      "\n",
      "[10414 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Predict for the next 5 years\n",
    "parent_dir = \"../data/curated/2023_2027_data\"\n",
    "\n",
    "for filename in os.listdir(parent_dir):\n",
    "    merged_df_yr = spark.read.csv(parent_dir + \"/\" + filename, header=True)\n",
    "\n",
    "    # Extract year from the file name \n",
    "    which_year = re.findall(r'\\d+', filename)\n",
    "\n",
    "    # Add year column to the dataset to fit the input into the model\n",
    "    merged_df_yr = merged_df_yr.withColumn(\"year\", lit(which_year[0]))\n",
    "\n",
    "    for c in merged_df_yr.columns:\n",
    "        if c not in [\"year\", \"sa2_2021\"]:\n",
    "            merged_df_yr = merged_df_yr.withColumn(c,merged_df_yr[c].cast(FloatType())) \n",
    "        elif c in [\"year\", \"sa2_2021\"]:\n",
    "            merged_df_yr = merged_df_yr.withColumn(c,merged_df_yr[c].cast(IntegerType())) \n",
    "\n",
    "    merged_df_yr = merged_df_yr.toPandas()\n",
    "    print(merged_df_yr[\"sa2_2021\"])\n",
    "    merged_df_yr.rename(columns = {'gdp(USD Millioins)':'gdp', 'saving_rate(% of GDP)':'saving_rate'}, inplace = True)\n",
    "    merged_df_yr['residence_type'] = merged_df_yr['residence_type'].astype('category')\n",
    "    merged_df_yr['residence_type'] = merged_df_yr['residence_type'].cat.codes\n",
    "    merged_df_yr.dropna(inplace=True)\n",
    "\n",
    "    # Reorder the columns \n",
    "    merged_df_yr_reordered = merged_df_yr[['year', 'sa2_2021', 'residence_type', 'nbed', 'nbath', 'ncar',\n",
    "       'min_distance_to_cbd', 'min_distance_to_park', 'min_distance_to_prim',\n",
    "       'min_distance_to_second', 'min_distance_to_train',\n",
    "       'min_distance_to_hosp', 'min_distance_to_poli', 'min_distance_to_shop',\n",
    "       'gdp', 'saving_rate', 'income_per_person', 'population_density',\n",
    "       'crime_cases']]\n",
    "\n",
    "    # Predict with random forest tree\n",
    "    prediction = sel_pred.predict(merged_df_yr_reordered)\n",
    "    prediction = np.exp(prediction)\n",
    "    new_csv_name = \"../data/curated/random_forest_pred/\" + filename\n",
    "    print(merged_df_yr_reordered['sa2_2021'].astype(int))\n",
    "    data = {'year': merged_df_yr_reordered['year'],\n",
    "            'sa2_2021': merged_df_yr_reordered['sa2_2021'],\n",
    "            'predicted_price': prediction }\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df)\n",
    "    df.to_csv(new_csv_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
