{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import IntegerType\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import lit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new folder for random forest preiction for rental price\n",
    "# Directory\n",
    "directory = \"random_forest_pred\"\n",
    "  \n",
    "# Parent Directory path\n",
    "parent_dir = \"../data/curated/\"\n",
    "\n",
    "# Path\n",
    "path = os.path.join(parent_dir, directory)\n",
    "\n",
    "# Create the directory\n",
    "os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '../data/curated/final_random_forest_pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/hyunjinp/generic-real-estate-consulting-project-group-43/models/randomForest_yearly.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/hyunjinp/generic-real-estate-consulting-project-group-43/models/randomForest_yearly.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(parent_dir, directory)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/hyunjinp/generic-real-estate-consulting-project-group-43/models/randomForest_yearly.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Create the directory\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/hyunjinp/generic-real-estate-consulting-project-group-43/models/randomForest_yearly.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m os\u001b[39m.\u001b[39;49mmkdir(path)\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '../data/curated/final_random_forest_pred'"
     ]
    }
   ],
   "source": [
    "# Make a new folder for random forest preiction for rental price\n",
    "# Directory\n",
    "directory = \"final_random_forest_pred\"\n",
    "  \n",
    "# Parent Directory path\n",
    "parent_dir = \"../data/curated/\"\n",
    "\n",
    "# Path\n",
    "path = os.path.join(parent_dir, directory)\n",
    "\n",
    "# Create the directory\n",
    "os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Tutorial 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "parent_dir = \"../data/curated/merged_dataset\"\n",
    "\n",
    "\n",
    "sel = RandomForestRegressor(n_estimators = 100)\n",
    "\n",
    "for filename in os.listdir(parent_dir):\n",
    "    if filename != \"2022_merged_data.csv\":\n",
    "        merged_df_yr = spark.read.csv(parent_dir + \"/\" + filename, header=True)\n",
    "\n",
    "        merged_df_yr = merged_df_yr.drop(\"address\",\"latitude\",\"longitude\",\"postcode\",\"sa2_2016\")\n",
    "        \n",
    "        for c in merged_df_yr.columns:\n",
    "            if (c not in  ['address', 'residence_type']):\n",
    "                merged_df_yr = merged_df_yr.withColumn(c,merged_df_yr[c].cast(FloatType())) \n",
    "\n",
    "        merged_df_yr = merged_df_yr.toPandas()\n",
    "\n",
    "        merged_df_yr['residence_type'] = merged_df_yr['residence_type'].astype('category')\n",
    "        merged_df_yr['residence_type'] = merged_df_yr['residence_type'].cat.codes\n",
    "\n",
    "        merged_df_yr.iloc[:, 13:21] = merged_df_yr.iloc[:, 13:21].replace(np.nan, 99999)\n",
    "\n",
    "        merged_df_yr.rename(columns = {'gdp(USD Millioins)':'gdp', 'saving_rate(% of GDP)':'saving_rate'}, inplace = True)\n",
    "        \n",
    "        merged_df_yr = merged_df_yr.dropna()\n",
    "   \n",
    "        X = merged_df_yr.drop(['weekly_rent'], axis=1)\n",
    "        y = np.log(merged_df_yr['weekly_rent'])\n",
    "        # Train with the whole dataset for the actual prediction for the next 5 years (This training is not for feature enginnering & accuracy test)\n",
    "        sel.fit(X, y)\n",
    "\n",
    "accuracy_input = spark.read.csv(parent_dir + \"/\" + \"2022_merged_data.csv\", header=True)\n",
    "accuracy_input = accuracy_input.drop(\"address\",\"latitude\",\"longitude\",\"postcode\",\"sa2_2016\")\n",
    "        \n",
    "for c in accuracy_input.columns:\n",
    "    if (c not in  ['address', 'residence_type']):\n",
    "        accuracy_input = accuracy_input.withColumn(c,accuracy_input[c].cast(FloatType())) \n",
    "\n",
    "accuracy_input = accuracy_input.toPandas()\n",
    "\n",
    "accuracy_input['residence_type'] = accuracy_input['residence_type'].astype('category')\n",
    "accuracy_input['residence_type'] = accuracy_input['residence_type'].cat.codes\n",
    "\n",
    "accuracy_input.iloc[:, 13:21] = accuracy_input.iloc[:, 13:21].replace(np.nan, 99999)\n",
    "\n",
    "accuracy_input.rename(columns = {'gdp(USD Millioins)':'gdp', 'saving_rate(% of GDP)':'saving_rate'}, inplace = True)\n",
    "        \n",
    "accuracy_input = accuracy_input.dropna()\n",
    "   \n",
    "X_accuracy = accuracy_input.drop(['weekly_rent'], axis=1)\n",
    "y_accuracy_true = np.log(accuracy_input['weekly_rent'])\n",
    "\n",
    "prediction = sel.predict(X_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41159.80161694951\n",
      "202.87878552709623\n",
      "Mean Absolute Error: 102.86 degrees.\n",
      "Accuracy: 83.06 %.\n"
     ]
    }
   ],
   "source": [
    "# Check the accuracy before predicting with the trained random forest regressor\n",
    "y_accuracy_true = np.exp(y_accuracy_true)\n",
    "prediction = np.exp(prediction)\n",
    "mse = mean_squared_error(y_accuracy_true, prediction)\n",
    "rmse = mse**.5\n",
    "print(mse)\n",
    "print(rmse)\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = np.abs(prediction - y_accuracy_true)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', np.round(np.mean(errors), 2), 'degrees.')\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / y_accuracy_true)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', np.round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2021_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2020_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2022_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2018_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2014_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2017_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2015_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2019_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n",
      "2016_merged_data.csv ----------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Tutorial 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "parent_dir = \"../data/curated/merged_dataset\"\n",
    "\n",
    "sel_pred = RandomForestRegressor(n_estimators = 100)\n",
    "\n",
    "for filename in os.listdir(parent_dir):\n",
    "    print(filename, \"----------------------------------------------------------------------------------------------------------------\")\n",
    "    merged_df_yr = spark.read.csv(parent_dir + \"/\" + filename, header=True)\n",
    "\n",
    "    merged_df_yr = merged_df_yr.drop(\"address\",\"latitude\",\"longitude\",\"postcode\",\"sa2_2016\")\n",
    "    \n",
    "    for c in merged_df_yr.columns:\n",
    "        if (c not in  ['address', 'residence_type']):\n",
    "            merged_df_yr = merged_df_yr.withColumn(c,merged_df_yr[c].cast(FloatType())) \n",
    "\n",
    "    merged_df_yr = merged_df_yr.toPandas()\n",
    "\n",
    "    merged_df_yr['residence_type'] = merged_df_yr['residence_type'].astype('category')\n",
    "    merged_df_yr['residence_type'] = merged_df_yr['residence_type'].cat.codes\n",
    "\n",
    "    merged_df_yr.iloc[:, 13:21] = merged_df_yr.iloc[:, 13:21].replace(np.nan, 99999)\n",
    "\n",
    "    merged_df_yr.rename(columns = {'gdp(USD Millioins)':'gdp', 'saving_rate(% of GDP)':'saving_rate'}, inplace = True)\n",
    "    \n",
    "    merged_df_yr = merged_df_yr.dropna()\n",
    "\n",
    "    \n",
    "    X = merged_df_yr.drop(['weekly_rent'], axis=1)\n",
    "    y = np.log(merged_df_yr['weekly_rent'])\n",
    "    # Train with the whole dataset for the actual prediction for the next 5 years (This training is not for feature enginnering & accuracy test)\n",
    "    sel_pred.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for the next 5 years\n",
    "parent_dir = \"../data/curated/2023_2027_data\"\n",
    "\n",
    "for filename in os.listdir(parent_dir):\n",
    "    merged_df_yr = spark.read.csv(parent_dir + \"/\" + filename, header=True)\n",
    "\n",
    "    # Extract year from the file name \n",
    "    which_year = re.findall(r'\\d+', filename)\n",
    "\n",
    "    # Add year column to the dataset to fit the input into the model\n",
    "    merged_df_yr = merged_df_yr.withColumn(\"year\", lit(which_year[0]))\n",
    "\n",
    "    for c in merged_df_yr.columns:\n",
    "        merged_df_yr = merged_df_yr.withColumn(c,merged_df_yr[c].cast(FloatType())) \n",
    "\n",
    "    merged_df_yr = merged_df_yr.toPandas()\n",
    "    merged_df_yr.rename(columns = {'gdp(USD Millioins)':'gdp', 'saving_rate(% of GDP)':'saving_rate'}, inplace = True)\n",
    "    merged_df_yr['residence_type'] = merged_df_yr['residence_type'].astype('category')\n",
    "    merged_df_yr['residence_type'] = merged_df_yr['residence_type'].cat.codes\n",
    "    merged_df_yr.dropna(inplace=True)\n",
    "\n",
    "    # Reorder the columns \n",
    "    merged_df_yr_reordered = merged_df_yr[['year', 'sa2_2021', 'residence_type', 'nbed', 'nbath', 'ncar',\n",
    "       'min_distance_to_cbd', 'min_distance_to_park', 'min_distance_to_prim',\n",
    "       'min_distance_to_second', 'min_distance_to_train',\n",
    "       'min_distance_to_hosp', 'min_distance_to_poli', 'min_distance_to_shop',\n",
    "       'gdp', 'saving_rate', 'income_per_person', 'population_density',\n",
    "       'crime_cases']]\n",
    "\n",
    "    # Predict with random forest tree\n",
    "    prediction = sel_pred.predict(merged_df_yr_reordered)\n",
    "    prediction = np.exp(prediction)\n",
    "    new_csv_name = \"../data/curated/random_forest_pred/\" + filename\n",
    "\n",
    "    np.savetxt(new_csv_name,  np.c_[merged_df_yr_reordered['year'].astype(int),merged_df_yr_reordered['sa2_2021'].astype(int),prediction], delimiter=\",\", header=\"year,sa2_2021,weekly_rent\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
