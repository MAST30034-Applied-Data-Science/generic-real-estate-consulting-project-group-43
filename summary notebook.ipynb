{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Real Estate Consulting Project\n",
    "Group number:43 <br/>\n",
    "Group members:  <br/>\n",
    "Jiahe Liu (jiahe3@student.unimelb.edu.au) <br/>\n",
    "Hyunjin Park (hyunjinp@student.unimelb.edu.au) <br/>\n",
    "Jongho Park (jonghop@student.unimelb.edu.au) <br/>\n",
    "Nuo Chen (nc1@student.unimelb.edu.au) <br/>\n",
    "Anzhe Cai (anzhec@student.unimelb.edu.au) <br/>\n",
    " \n",
    "* Note: file names beginning with \"test_\" are approaches we tried but seemed not useful or we found alternative ways to optimise\n",
    " \n",
    "**This notebook is a summary of the following topics:**\n",
    "1. The features you used\n",
    "2. The Models you fit\n",
    "3. The results you achieved\n",
    "4. Any issues you ran into\n",
    "5. The assumptions that you made.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers to the topic questions \n",
    "\n",
    "**1. What are the most important internal and external features in predicting rental prices?**\n",
    " \n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Model</th>\n",
    "        <td>Selected features</td>\n",
    "        <td>Notes</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>OLS</th>\n",
    "      <td>'min_distance_to_cbd','min_distance_to_poli', 'min_distance_to_shop', 'min_distance_to_hosp', 'min_distance_to_park', 'min_distance_to_prim', 'min_distance_to_second', 'min_distance_to_train', 'nbath', 'nbed', 'ncar', GDP, crime cases, saving rate, population density, income per person, residence type, sa2 code and year.</td>\n",
    "      <td>distance to secondary school excluded by the forward selection with lowest AIC.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <th>XGBoost</th>\n",
    "    <td>'min_distance_to_cbd', 'min_distance_to_hosp', 'min_distance_to_park', 'min_distance_to_prim', 'min_distance_to_second', 'min_distance_to_train', 'nbath', 'nbed', 'ncar', GDP, crime cases, saving rate, population density, income per person, residence type, sa2 code and year.</td>\n",
    "    <td>distance to shopping mall and police station are excluded by the forward selection with lowest MSE.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Random Forest</th>\n",
    "      <td>All features were selected after the forward selection using MSE from a random forest tree.</td>\n",
    "      <td>By referring to the overview of the features, all the internal features and the distance features from the external are assumed not to be changed in the next 5 years (2023 - 2027).</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "</table>\n",
    "\n",
    "**2. What are the top 10 suburbs with the highest predicted growth rate?**\n",
    "  <table>\n",
    "  <tr>\n",
    "  <th>rank</th>\n",
    "  <th>suburb</th>\n",
    "  <th>sa2 2021 code</th>\n",
    "  <th>averaged growth rate(%) 2023-2027</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>Narre Warren North</td>\n",
    "      <td>212021299</td>\n",
    "      <td>14.75</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>Point Cook - East</td>\n",
    "      <td>213051464</td>\n",
    "      <td>14.16</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>Elwood</td>\n",
    "      <td>206051129</td>\n",
    "      <td>10.16</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>Yarriambiack</td>\n",
    "      <td>215011394</td>\n",
    "      <td>9.96</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>Nhill Region</td>\n",
    "      <td>215011390</td>\n",
    "      <td>8.92</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>Hawthorn East</td>\n",
    "      <td>207011152</td>\n",
    "      <td>8.55</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>6</th>\n",
    "      <td>Kerang</td>\n",
    "      <td>215031402</td>\n",
    "      <td>8.37</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>7</th>\n",
    "      <td>St Kilda - Central</td>\n",
    "      <td>206051513</td>\n",
    "      <td>7.70</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>8</th>\n",
    "      <td>St Kilda - West</td>\n",
    "      <td>206051514</td>\n",
    "      <td>7.69</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>9</th>\n",
    "      <td>Frankston</td>\n",
    "      <td>214011371</td>\n",
    "      <td>7.49</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>10</th>\n",
    "      <td>Malvern East</td>\n",
    "      <td>208041195</td>\n",
    "      <td>7.37</td>\n",
    "    </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. What are the most liveable and affordable suburbs according to your chosen metrics?**\n",
    "\n",
    "By default, i.e. if we don’t put any weight for each factor (eg. shortest distance to primary schools, parks etc.), the most liveable suburbs & properties are \n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>The top 5 recommended suburbs (with liveability score out of 100)</th>\n",
    "      <th>The top 5 recommended properties(with liveability score out of 100)</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>1. Box Hill, 72.04</td>\n",
    "      <td>1. 703/703/4-6 STATION STREET, MOORABBIN , Apartment, $430.0 per week, 87.61</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>2. Fitzroy, 70.6</td>\n",
    "      <td>2. G04/710 STATION STREET, BOX HILL , Apartment, $370.0 per week, 84.34</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>3. Brighton (Vic.), 69.94</td>\n",
    "      <td>3. 207/2 ELLAND AVENUE, BOX HILL , Apartment, $365.0 per week, 84.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>4. Footscray, 69.68</td>\n",
    "      <td>4. 403/2-4 ELLAND AVENUE, BOX HILL , Apartment, $390.0 per week, 84.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>5. Williamstown, 69.47</td>\n",
    "      <td>5. 1A BOLTON STREET, BEAUMARIS , Apartment, $495.0 per week, 83.53</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Downloading\n",
    "\n",
    "\n",
    "## 1.1 Property Dataset Downloading\n",
    " \n",
    "We retrieved data from `Domain.com` and `oldlisting.com` using API and web scraping respectively. We also applied web scraping to retrieve data from `Domain.com`. However, we realised that they are similar from API-retrieved ones and Web-scrapped Domain data contained more error messages and will result in big data loss after data cleaning. Hence, we decided to use API-retrieved Domain data.\n",
    " \n",
    "0. `/notebooks/Scraping/0.BS_Scraping.ipynb`: \n",
    " \n",
    "**Approach:** This notebook will scrape data at Domain using beautiful soup. However, these data will not be used in preliminary analysis or modelling to avoid redundancy.\n",
    " \n",
    "**Assumption & Limitation:**  We assume that residential data from web scraping might show different or hidden data compared to the data retrieved by API. So we decided to use both web scraping and API methods to retrieve the data and compare between them.\n",
    " \n",
    "**conclusion:** It shows the data from API and web scraping are exactly the same but data from API are more stable for the number of baths and beds. So we decided to use the data from the API.\n",
    " \n",
    "1. `/notebooks/Scraping/1.API_Retrieving.ipynb`:  \n",
    " \n",
    "**Approach:** This notebook will retrieve data at Domain using API.However, as you may need to apply for your own client key and secret via https://developer.domain.com.au/ . If you have questions regarding this, please contact the team via jiahe3@student.unimelb.edu.au\n",
    " \n",
    "**Assumption & Limitation:** This data is imbalanced. Most of the data is on 2022 and the rest of few data are historical data, we assume that the domain.com limited the API access of historical data.\n",
    " \n",
    "2. `/notebooks/Scraping/2.Get_historical_url.ipynb`:\n",
    " \n",
    "**Approach:** This notebook will get all links of rental records per postcode from: https://www.oldlistings.com.au/site-map?page=17&state=VIC. This notebook  will generate all the URL links that contains historical rental information into `/notebooks/Scraping/HIS_url_links`\n",
    "It will generate the historical data retrieved from the first 100 URLs for example demonstration. However, it is very likely to be blocked if we generate too much data at each time. Hence, the team divided the web scraping tasks and generated them gradually using the notebook script. \n",
    " \n",
    "**Assumption & Limitation:** We assume that we can retrieve all data from oldlisting.com with all of these URLs links. \n",
    " \n",
    "3. `/notebooks/Scraping/3.Group_scraping_tasks.ipynb`: \n",
    " \n",
    "**Approach:** This notebook will generate all historical data (2006-2022) retrieved from old listing to `/data/raw/historical_data/`.We retrieved around 572k data from oldlisting.com.\n",
    " \n",
    "**Assumption & Limitation:** Although we’ve put `timesleep` within our code, we experienced blockings from the website. We assumed that this happened the website may cumulatively count the number of times we made requests. Hence, we made some improvements. First improvement was to sparse the scraping work. We did the scrapping day by day with a smaller amount of web scraping so we can finish this task within the scheduled timeline. Another improvement was to use AWS to initiate a virtual environment and change IP while doing the retrieving.\n",
    " \n",
    "4. `/notebooks/Scraping/4.Scraping_whole.ipynb`: \n",
    " \n",
    "**Approach:** This notebook will use an api-rotator through AWSAs to prevent web scraping from being blocked. The historical website monitors IP addresses with high levels of activity, we assume the website counts the number of access by trying many accesses with reasonable breaks and checking when it gets blocked. So there are few solution options and one of them is using an api-rotator through AWS. It is shown in the file `/scripts/aws-scraping-tool.py`. The AWS API key will be hidden in the .env file.\n",
    " \n",
    "**Assumption & Limitation:** As the historical source website blocks the ip address, we assume that if we use a different ip address before the quota use we can scrape unlimited so we use AWS ip-rotator to scarpe the whole historical data.\n",
    " \n",
    "**overall findings:**\n",
    "  <table>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <td>Initial number of data</td>\n",
    "        <td>Dropping duplicates & non weekly-rent data</td>\n",
    "        <td>Filtering for residential property data</td>\n",
    "        <td>Number of outlier removed</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>API Retrieving</th>\n",
    "      <td>15k</td>\n",
    "      <td>15k → 14.6k (2.6% removed)</td>\n",
    "      <td>same as below</td>\n",
    "      <td>same as below</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Web Scraping</th>\n",
    "      <td>572k</td>\n",
    "      <td>572k → 339k (40.6% removed)</td>\n",
    "      <td>same as below</td>\n",
    "      <td>same as below</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Total</th>\n",
    "      <td>587k</td>\n",
    "      <td>587k → 345k (41.2% removed)</td>\n",
    "      <td>345k → 282k (18.2% removed)</td>\n",
    "      <td>282k ->231k (18% removed; 1-3% removed per year)</td>      \n",
    "    </tr>\n",
    "    <tr>\n",
    "</table>\n",
    "\n",
    "## 1.2 External Dataset Downloading\n",
    "**Approach:** We retrieved data of external attribute by urls, such as crime cases, total personal income, estimated resident population, GDP, saving rate from `abs.gov.au`, `hcrimestatistics.vic.gov.au` and `data.oecd.org/natincome/saving-rate.htm`. The notebook script `/notebooks/External/0.external_dataset_download.ipynb` is used to download those external dataset, and store them into the raw folder in the data folder.\n",
    " \n",
    "**Assumption & Limitation:** We assume that the data is up to date and represents most of the facilities in Victoria.\n",
    " \n",
    " \n",
    "## 1.3 External Dataset Downloading \n",
    "(Infrastructure Facility Locations Retrieved by API)`/notebooks/Scraping/5.facility_data_retreiving`: \n",
    " \n",
    "**Approach:** We retreieved data of features of interest in Victoria state by retreiving query in GeoJson format through `https://services6.arcgis.com/GB33F62SbDxJjwEL/ArcGIS/rest/services/Vicmap_Features_of_Interest/FeatureServer/8/query`. The selected features of interest that we decided to include were primary/secondary schools, parks, train stations, hospitals, market places, police stations and shopping malls. By querying them by each year with their registered data on the dataset of `https://www.arcgis.com/home/webmap/viewer.html?url=https://services6.arcgis.com/GB33F62SbDxJjwEL/ArcGIS/rest/services/Vicmap_Features_of_Interest/FeatureServer/8&source=sd`, we could retrieve each year's features of interest locations that is used to calculate the distance between a feature of interest and a rental property from the historical rental property dataset.\n",
    " \n",
    "### Structure of features of interest dataset for each year\n",
    "The way of producing one feature of interest for a particular year is cumulative.\n",
    "eg) To retrieve the data for **marketplaces in 2014**, all the marketplaces registered before 2013 + marketplaces registered in 2013 + marketplaces registered in 2014 are combined. <br/>\n",
    "### The reasons of choosing these particular features\n",
    "We have chosen these 8 features of interest to put into our model as inputs out of many other features of interest in Victoria, since we consider them to have a significant relevance to livability, which is one of the main topic questions. According to Global livability Index 2021 Report `https://www.eiu.com/n/campaigns/global-liveability-index-2021/`, they stated 5 factors that compose liveability as \"stability, healthcare, culture and environment, education and infrastructure\".\n",
    "**By referring to these five factors, we have chosen the 8 features with the following reasons:** <br/>\n",
    "- Primary/secondary schools: Education\n",
    "- Parks, Shopping malls: Culture and environment\n",
    "- Hospitals: Healthcare\n",
    "- Markets, Police stations, Train stations: Stability, Infrastructure\n",
    " \n",
    "**Assumption & Limitation:** The registered dates seem to be not accurate enough. For example, from this dataset, some train stations that are registered in recent years (2013-2022) were actually built in the 1900s and are not being used anymore. However, since it is impossible to manually search up for every one of these types of features and remove them from the dataset, our team decided to put this as a limitation of our dataset.\n",
    "We have collected market locations in victoria state, but entire market place instances are only 7 which seems to be very little to the actual number of market places in Victoria. Also, since the number of instances is too small, it is hard to expect the distances between a property and the nearest market place to be a significant variable to predict weekly rent. Therefore, the marketplace folder was not used.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "# 2. Data Preprocessing\n",
    " \n",
    "## Overview of the features\n",
    "### Internal factors are:\n",
    "1. The residence type of a rental property \n",
    "2. The number of bedrooms of a rental property \n",
    "3. The number of bathrooms of a rental property\n",
    "4. The number of car parking spaces of a rental property\n",
    " \n",
    "### External factors are: \n",
    "1. Year\n",
    "2. SA2_2021 \n",
    "3. Minimum distance from a rental property to cbd / a closest park / primary&secondary schools / train station / hospital / police office / shopping mall \n",
    "4. Yearly GDP of each SA2 codes (Assumptions made that Australian GDP will apply the same to all the suburbs in SA2 granularity) \n",
    "5. Yearly Saving rate of each SA2 codes (Assumptions made that Australian saving rates will apply the same to all the suburbs in SA2 granularity) \n",
    "6. Yearly Income per person by each SA2 region\n",
    "7. Yearly Crime cases by each SA2 codes (postcode -> SA2 codes after merging)\n",
    "8. Yearly Population density by each SA2 region\n",
    " \n",
    "## 2.1 Property Dataset Preprocessing\n",
    " \n",
    "### 2.1.1 API-retrieved Data Preprocessing (rental price)\n",
    " \n",
    "0. `/notebooks/Preprocessing/0.API_prepro.ipynb`: \n",
    " \n",
    "**Approach:** This notebook will preprocess API data rental price. We kept the integer only for the rental price. This notebook also converted the API dataset to the same format as the Web Scraped dataset for future concatenation.\n",
    " \n",
    "**Assumption & Limitation:** We assume the data with following regular expressions are weekly rents.\n",
    " \n",
    "*   --TYPE1-- **price value with `'w'`**:\n",
    "   *   The common characteristic between 'pw', 'per week', 'weekly' etc. is that they all have 'w' in their unit. Thus by extracting the price followed by w, we can avoid extracting monthly rent.\n",
    "*   --TYPE2--  **price value only**:\n",
    "   *   We assume if the price description contains value only, it is the weekly rent.\n",
    "*   --TYPE3-- **price value ended with `'d'`**:\n",
    "   *   From experiments, we discovered that there are many instances which didn't specify weekly rent however specified 'furnished' eg`$490 Fully Furnished`, we will assume these as weekly rent.\n",
    "*   --TYPE4--**price value with `**description**`**:\n",
    "   *   We also discovered many price looked like `$750 **SPACIOUS APARTMENT**`, hence we assume those are weekly rent.\n",
    " \n",
    " \n",
    "### 2.1.2 Web-Scraping-retrieved Data Preprocessing (rental price, outlier detection for all features)\n",
    " \n",
    "1. `/notebooks/Preprocessing/1.History_prepro.ipynb`:\n",
    "**Approach:** This notebook will preproces Web Scraping + API data preprocessing. We firstly cleaned Web-Scraped data's rental price and then combined it with API datasets. We classified the whole dataset and re-saved them by year. This is because there may be inflations every year and the variance of rental price calculated year by year may be more reasonable.\n",
    " \n",
    "**Assumption & Limitation:** For the number of rooms, we viewed the boxplot and manually confirmed some intuitive outliers (large number of rooms, very low price). For the location (latitude and longitude), we amended 3 badly scraped location data.\n",
    " \n",
    "For the rental price, we removed outliers year by year. For each year, we removed the rental price values outside of the 3 standard deviations as they were assumed as outliers. Each year, around 1 to 3 % of data were removed as outliers. Hence we suggest it is a reasonable outlier removal approach.\n",
    "\n",
    "**Overall findings**\n",
    "We removed these number of outliers for each year\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>year</th>\n",
    "      <th>data removed</th>\n",
    "      <th>data left</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>2006</td>\n",
    "      <td>11 (2.1%)</td>\n",
    "      <td>512</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>2007</td>\n",
    "      <td>75 (1.59%)</td>\n",
    "      <td>4633</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>2008</td>\n",
    "      <td>75 (1.59%)</td>\n",
    "      <td>5396</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>2009</td>\n",
    "      <td>98 (1.78%)</td>\n",
    "      <td>5078</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>2010</td>\n",
    "      <td>99 (1.59%)</td>\n",
    "      <td>6133</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>2011</td>\n",
    "      <td>135 (1.53%)</td>\n",
    "      <td>8680</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>6</th>\n",
    "      <td>2012</td>\n",
    "      <td>200 (1.8%)</td>\n",
    "      <td>10917</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>7</th>\n",
    "      <td>2013</td>\n",
    "      <td>270 (2.28%)</td>\n",
    "      <td>11569</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>8</th>\n",
    "      <td>2014</td>\n",
    "      <td>251 (1.96%)</td>\n",
    "      <td>12538</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>9</th>\n",
    "      <td>2015</td>\n",
    "      <td>260 (1.97%)</td>\n",
    "      <td>12928</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>10</th>\n",
    "      <td>2016</td>\n",
    "      <td>340 (2.18%)</td>\n",
    "      <td>15266</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>11</th>\n",
    "      <td>2017</td>\n",
    "      <td>420 (2.37%)</td>\n",
    "      <td>17331</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>12</th>\n",
    "      <td>2018</td>\n",
    "      <td>443 (2.18%)</td>\n",
    "      <td>19877</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>13</th>\n",
    "      <td>2019</td>\n",
    "      <td>549 (2.45%)</td>\n",
    "      <td>21851</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>14</th>\n",
    "      <td>2020</td>\n",
    "      <td>607 (2.69%)</td>\n",
    "      <td>21987</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>15</th>\n",
    "      <td>2021</td>\n",
    "      <td>770 (2.77%)</td>\n",
    "      <td>27035</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>16</th>\n",
    "      <td>2022</td>\n",
    "      <td>2360 (3.11%)</td>\n",
    "      <td>73639</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "### 2.1.3 Adding SA2 Code for Cleaned Data\n",
    "**Overall Approach:** There are many options to add SA2.\n",
    "First we add SA2 using suburb name as generally SA2 name refers to suburb in the residential address (see document  https://www.abs.gov.au/ausstats/abs@.nsf/lookup/by%20subject/1270.0.55.001~july%202016~main%20features~statistical%20area%20level%202%20(sa2)~10014) however there are many of them fail to match with SA2 name because of different formats, some of region added prefix or suffix (ex. -south, -north, the great-, etc...) and the few regions that have totally different name to SA2 name. \n",
    " \n",
    "Second, we use location data to add SA2. This approach is genuine method as the all of data has location meta data in the website and API and the data loss is less than 0.5% (few data is located outside of victoria) and easy to process. For the process, we use shap file from ABS (https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files) and map polygon coordinates and check which region contains the residence locations. This will assign SA2 code to every residential locations. \n",
    " \n",
    "2. `/notebooks/Preprocessing/2.add_SA2_coordinate_API.`: \n",
    " \n",
    "**Approach:** This notebook will add 2021 SA2 code for each property instance retrieved by Domain API. This is done by checking whether the location point (longitude, latitude) is within the SA2 polygon from the shape file.\n",
    " \n",
    "**Assumption & Limitation:** There were a few data losses. We assume these data losses were reasonable because the coordinates point to outside of Victoria when we plot the unmatched data to the map. Another limitation for this is we used brutal force to check whether one location is within a shape file, hence running the function may take long.\n",
    " \n",
    "3. `/notebooks/Preprocessing/3.add_SA2_coordinate_WS_and_facilities`:\n",
    " \n",
    "**Approach:** This notebook will add 2021 SA2 code for each property instance retrieved oldlisting.com web scraping and facility (school, parks, hospital etc.) dataset. Approaches are the same as the above.\n",
    " \n",
    "**Assumption & Limitation:** same as above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "### 2.1.4 Adding Distance/Time to Places/CBD from Clean Property Data (Open Route Service API)\n",
    "4. `/notebooks/Preprocessing/4.ors_add_rentalDistance.ipynb`. \n",
    " \n",
    "**Approach:**  This notebook will add distance between each property and its facilities. At each iteration, a yearly subset of places csv and a year subset of property csv between 2013-2021 were called out and merged based on SA2 Code. Then, a list of clients registered with unique API keys was used to request distance/time for each merged yearly dataset.\n",
    " \n",
    "This section uses private API keys, which can be requested on https://openrouteservice.org/plans/.\n",
    " \n",
    "In the add_distance_time function, the requests were conducted by iterating SA2 codes. All properties in current SA2 code were computed as sources, each of which was mapped to all places defined by this SA2 code. So in total, (size of sources * count of places) routes of distances/time were computed for each SA2 district.\n",
    " \n",
    "For route number greater than 3500 which was prohibited by ORS, a slicing method that divided sources into smaller subsets based on the factor, by which route number exceeded 3500, replaced the original requests. Through this, the restriction could be solved by making additional calls for reduced sublists. For example for 206 sources * 35 places = 7210 > 3500, exceeding by a factor of 2, sources list was divided into \\[0-68), \\[68-136) and \\[136-206) sublists, and 3 requests with route number 68 * 35, 68 * 35 and 70 * 35 were called to ensure all 7210 data was retrieved.\n",
    " \n",
    "For exceptions such as exceeding API quota during processing, a back up API key with a capacity of 2500 calls was switched in to handle the exception. For other keys, the individual quota of 500 was mostly sufficient to process each merged yearly dataset given there were fewer than 500 SA2 codes in each year (except for 502 in 2022). \n",
    " \n",
    "**Assumption & Limitation:** Only includes driving distances instead of walking distances. Limitation includes the quota of 500 API requests per day, the call rate of 40 per minute, which slows down requests.\n",
    " \n",
    "5. `/notebooks/Preprocessing/5.ors_iteration_add_rentalDistance.ipynb`: \n",
    " \n",
    "**Approach:** This notebook will perform the mentioned steps iteratively to retrieve the (2013-2022) data we need. `/notebooks/Preprocessing/4.ors_add_rentalDistance.ipynb`: this notebook will retrieve 2022 data individually due to large data size.\n",
    " \n",
    "**Assumption & Limitation:** Assume each API key is fully refreshed at the time of requesting. Network issues may occur during iterative API retrieval, which slows down requests even more.\n",
    " \n",
    "6. `/notebooks/Preprocessing/6.get_minDistance.ipynb`: \n",
    " \n",
    "**Approach:** This notebook will compare distances to each facility within a SA2 suburb for each property and only keep the minimal distance i.e. distance to the closest facility. NA value will be applied if there is no type of facility within a SA2 suburb. These NA values will be filled later by the maximum number within a column.\n",
    " \n",
    "**Assumption & Limitation:** Assume the unavailable distances (due to missing places) means the property is very ‘far’ to certain places. This might not be true since a property can be close to a place, which is just unavailable during data retrieval. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 Feature Engineering for Property Dataset\n",
    " \n",
    "**Approach:** We engineered `residence type` as a binary feature (House or Apartment). This is because this project analyses residential properties, hence we excluded rental properties for office, holiday, rural area etc. We classified the rest residence type with logistic regression into `House` including townhouse, terrace, etc. and `Apartment` including unit, studio etc. for simplification so that we can ask our client for their rental residence type preference.\n",
    " \n",
    "However,To view these steps please see `/models/classify_property_type.ipynb`.\n",
    " \n",
    "**Assumption & Limitation:** \n",
    "The reason for using logistic regression model is because our model satisfies the following assumptions of logistic regression:\n",
    " \n",
    "1. output class: residence type is binary (1: House, 0: Apartment).\n",
    " \n",
    "2. observations are independent of each other: each rental record is independent of other records.\n",
    " \n",
    "3. independent variables are not to be too highly correlated with each other.\n",
    " \n",
    "4. logistic regression assumes linearity of independent variables and log odds.\n",
    " \n",
    "5. large sample size.\n",
    " \n",
    "* we assume that properties with the following labels are houses \n",
    "`   '- House',\n",
    "   'Cottage',\n",
    "   'Duplex',\n",
    "   'Duplex Or Semi',\n",
    "   'Duplex/semi Detach',\n",
    "   'Duplexsemi',\n",
    "   'Duplexsemi-detached',\n",
    "   'House',\n",
    "   'Houses',\n",
    "   'Residential House',\n",
    "   'Semi',\n",
    "   'Semi Detached',\n",
    "   'Semi-detached',\n",
    "   'Semi-detached/duplex',\n",
    "   'Semi-detached',\n",
    "   'Semi-duplex',\n",
    "   'Townhouse',\n",
    "   'Villa',\n",
    "   'Villa, House',\n",
    "   'Villa, Unit, House',\n",
    "`\n",
    "* we assume that properties with the following labels are apartment \n",
    "`'2 Storey Unit',\n",
    "'Apartment',\n",
    "'Block Of Units',\n",
    "'Block Units',\n",
    " 'Flat',\n",
    "'Flat, Block Of Units',\n",
    "'Home Unit',\n",
    "'Studio',\n",
    "'Unit',\n",
    "`\n",
    "* we assume that properties with the following labels are other type \n",
    "`'Available',\n",
    "'Available Now',\n",
    "'Rental',\n",
    "'Rental Property',\n",
    "'Rental_residential',\n",
    "'Residential',\n",
    "'Residential Home',\n",
    "'Residential House',\n",
    "'Residential Lease',\n",
    "'Residential Rentals',\n",
    "'none'`\n",
    " \n",
    " \n",
    "* we discovered that in total 42.66% of properties (231497 property data in total) do not have a residence type label.We assume that these properties are either houses or apartments. Hence, made a classification model to label them as either House or Apartment.\n",
    "​​\n",
    "## 2.2 External Dataset Preprocessing\n",
    "**Approach:** The notebook script `/notebooks/External/1.external_preprocess.ipynb` is used to do preprocessing on those external dataset (estimated resident population, total income, crime cases, GDP, saving rate), and store into the curated folder in the data folder. We calculated the population density for each sa2 region (2021), income per person per year for each sa2 region (2016) based on the dataset of estimated resident population and total income respectively. We tried to convert the 2016 sa2 of income to 2021 sa2, but this would lead to lots of data missing. Therefore, we decided to use 2016 sa2 for later merging. There are also some plots for different types of offence.\n",
    " \n",
    "**Assumption & Limitation:** Assumption for income: later year will be used, i.e 2012-2013 will be assumed as 2013.\n",
    "## 2.3 Data Merging\n",
    " \n",
    "### 2.3.1 Adding SA2 2016\n",
    "**Approach:** There were issues that arose that the statistical data (including population, income, etc...) before 2021 had a different SA2 code standard established in 2016. Every five year the SA2 code standard gradually changes so we need to add more SA2 code standards to the instance. As the name and area of SA2 codes change, we need stable data to match the code. The using location data is decided as an fitable method because it is stable and can point directly what SA2 region it was used to belong to in the past.\n",
    "To view these steps please see `/notebooks/Preprocessing/7.add_SA2_2016.ipynb` and `/notebooks/Preprocessing/8.organising.ipynb`.\n",
    " \n",
    "**Assumption & Limitation:** As the external data is from 2013 to 2021, the standard of SA2 in some data is based on 2016. Therefore we assume that each of the houses and apartments remain the same coordinates whenever it is 2013 or 2021 and we decide to add SA2 2016 code using coordinates. This is the same as the previous methods.\n",
    " \n",
    "### 2.3.2 Adding External Features\n",
    " \n",
    "**Approach:** For merging external data of 2013 to 2022, the notebook script `/notebooks/External/2.external_merge.ipynb` is used to merge external attributes (GDP and saving rate, income per person for each sa2, population density and crime cases) with the data in the min_distance_sa2_organised folder in curated folder, and also drop months to get values of all attributes based on year. If the values of external attributes (GDP and saving rate, income per person for each sa2, population density and crime cases) are missing, the predicted values for the external attributes (in the features_prediction folder in curated folder) are used to merge.\n",
    " \n",
    "For merging predicted values of external attributes of 2023 to 2027, the notebook script `/notebooks/External/3.2023_2027_merge.ipynb` is used to merge predicted values of external attributes (GDP and saving rate, income per person for each sa2, population density and crime cases) with the postcode of Local Government Area (LGA), 2016 sa2 codes and 2021 sa2 codes from 2022 dataset. Since the 2022 dataset has the most values of postcode, sa2 2021 and sa2 2016, and includes all values from previous years of those attributes, postcode, sa2 2021 and sa2 2016 from 2022 dataset will be used for further prediction. The predicted values of the external attributes, that are used for merging, are from the features_prediction folder in the curated folder.\n",
    " \n",
    "`/notebooks/External/4.external_geo.ipynb`: this notebook visualises the population density of sa2 (2021) regions with geo plots.\n",
    " \n",
    "**Assumption & Limitation:** We assume that each property data can be attached with a 2016 and a 2021 sa2 code. We also assume that there will be no data loss during our merging steps because all of our external dataset are either based on 2016 or 2021 sa2 code.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Analysis\n",
    " \n",
    "## 3.1 Property Data Preliminary Analysis\n",
    "**Approach:** `/notebooks/preliminary_property_with_geo.ipynb`: this notebook performs analysis through houses and apartments together and respectively. Plots drawn by this were saved to `/plots/figure/`.\n",
    " \n",
    "**Assumption & Limitation:** The original geo shape file was too big and each html file will be up to 70MB size. Hence, we used a simplified geo shape file to improve graph-drawn efficiency. The simplification process was done through `https://mapshaper.org/`. `notebooks/Liveability/scoring.ipynb` has a clearer explanation of this. \n",
    " \n",
    "## 3.2 Property Data Geo Visual\n",
    " \n",
    "**Approach:** `/notebooks/preliminary_property_with_geo.ipynb`: This notebook also contains geo visual for houses and apartments per SA2 respectively.\n",
    " \n",
    "**Preliminary analysis will include:**\n",
    " \n",
    "1. visualising the number of houses vs number of apartments  (2013 - 2022)\n",
    "2. geo visual: total number of houses and apartments in each SA2 (2013 - 2022)\n",
    "3. geo visual: averaged weekly rent for apartments( 2013 - 2022)\n",
    "4. geo visual: averaged weekly rent for houses (2013 - 2022)\n",
    " \n",
    "**we will conclude our findings as the following:**\n",
    "1. number of houses are greater than number of apartments in our dataset\n",
    "2. apartments are mainly distributed in central victoria whilst houses are sparsely distributed across victoria\n",
    "3. For houses, the averaged weekly rent per sa2 peaked at central victoria and gradually decreased as it moved to the outer regions. This trend also applies to apartments but is not that obvious. The reason for this may be due to the lack of data.\n",
    " \n",
    "And the number of houses and apartments rental records per SA2 respectively. Geo plots were saved to `/plots/aggregated_geo`\n",
    " \n",
    "**Assumption & Limitation:** same as above\n",
    " \n",
    "# 4. Modelling\n",
    " \n",
    "## 4.1 Prediction Model for External Features\n",
    "**Overall Approach:** All feature prediction models mentioned below are in the `\\models` file. The output of predicted features' values will be stored into the features_prediction folder in the curated folder.\n",
    " \n",
    "**Overall Assumption & Limitation:** We assume there is a linear relationship between out predicting results and year. (eg. population density ~ year, income per person per sa2 ~ year,  crime cases per postcode ~ year, GDP ~ year, saving rates ~ year.)\n",
    " \n",
    "**Findings** We found that except for GDP and saving rate, the other feature modelling accuracy was high (up to 90%). The reason for this may be that GDP and saving rate has no sa2 code and year was the only feature. \n",
    " \n",
    "### 4.1.1 Population\n",
    "**Approach:** `/models/1a.bootstrap-population.ipynb`: this notebook predicts population from 2022 to 2027 using bootstrapping and linear regression, with `population density  ~ year + sa2 code(2021)`.\n",
    " \n",
    " \n",
    "### 4.1.2 Averaged income per person for each SA2\n",
    "**Approach:** The notebook script `/models/1b.income_predict.ipynb` is used to model the income per person for each sa2 (2016) with a linear regression model, and it is used to predict the income per person for each sa2 (2016) region from 2020 to 2027, with `income per person ~ year + sa2 code(2016)`.\n",
    "`year` is considered as numerical value and `sa2 code(2016)` is considered as categorical value.\n",
    " \n",
    "### 4.1.3 Crime cases per each postcode region\n",
    "**Approach:** `/models/1c.crimecase_predict.ipynb`:A linear regression model was used to predict the crime cases per each postcode region from 2023 to 2027.\n",
    "prediction formula: `crime cases ~ year + postcode`\n",
    "`year` was considered as numerical value and `postcode` was considered as categorical value\n",
    " \n",
    "### 4.1.4 GDP and Saving rate\n",
    "**Approach:** `/models/1d.gdp_saving_predict.ipynb`: this notebook predicts the GDP and saving rate from 2021 to 2027. The GDP is predicted by a linear regression model with `log(GDP) ~ log(year)`. And the saving rate is predicted by a quadratic regression model with `Saving rate ~ year^2`\n",
    " \n",
    "## 4.2 Rental Price Prediction\n",
    " \n",
    "**The assumptions made for Rental Price Prediction part:** \n",
    "The values of the internal predictors (residence type, number of bed/bathrooms and car spaces) won’t change for the next five years. \n",
    " \n",
    "### 4.2.1 OLS Model\n",
    "**Approach:** The basic testing on Ordinary Least Square Regression is performed in `notebooks/dataAnalysis.ipynb`. Please refer to this notebook in order to see what kind of testing we have gone through which will result in our future implementations in the following LR notebooks. </br>\n",
    "Ordinary Least Square Regression is implemented in `/models/LR_prediction_all.ipynb`. In this notebook, forward selection based on lowest AIC is first conducted, which takes around an hour to run. Then the resulting features are selected from the training set to train and test the model at a ratio of 7:3. Subsequently, the Linear Regression model in `/models/LR_future_prediction.ipynb` is trained by the full 2013-2022 merged dataset containing the selected features, which is then used to make future 2023-2027 predictions.\n",
    " \n",
    "**Features used:** distance to secondary school excluded by the forward selection with lowest AIC. \n",
    "Selected features are 'min_distance_to_cbd','min_distance_to_poli', 'min_distance_to_shop', 'min_distance_to_hosp', 'min_distance_to_park', 'min_distance_to_prim', 'min_distance_to_second', 'min_distance_to_train', 'nbath', 'nbed', 'ncar', GDP, crime cases, saving rate, population density, income per person, residence type, sa2 code and year.\n",
    " \n",
    "**Findings:** We predict the weekly rent price for each sa2 for 2023 - 2027 and output csv files to store the results. \n",
    " \n",
    "**Overall Assumption & Limitation:** Assume there is a linear relationship between the predictors and response variables. \n",
    " \n",
    "### 4.2.2 XGboost Model\n",
    "**Approach:** The XGboost model is used for the rent price prediction, referring to the `/models/rent_price_xgboost.ipynb`. Categorical values are encoded into numerical values. The holdout method is used for train test split and forward selection with lowest Mean Squared Error is used for feature selection. The XGboost regression model is trained, and the training accuracy is 0.742 while the test accuracy is 0.697. A feature importance graph is also plotted by XGboost built-in function. Further, this notebook predicts the rent prices for 2023 - 2027.\n",
    " \n",
    "**Features used:** distance to shopping mall and police station are excluded by the forward selection with lowest MSE. \n",
    "Selected features are 'min_distance_to_cbd', 'min_distance_to_hosp', 'min_distance_to_park', 'min_distance_to_prim', 'min_distance_to_second', 'min_distance_to_train', 'nbath', 'nbed', 'ncar', GDP, crime cases, saving rate, population density, income per person, residence type, sa2 code and year.\n",
    " \n",
    "**Findings:** We predict the weekly rent price for each sa2 for 2023 - 2027 and output csv files to store the results. \n",
    "\n",
    "**Overall Assumption & Limitation:** Due to the lack of historical data, the dataset is biassed. Therefore, we use xgboosting to deal with the biases of the dataset.\n",
    " \n",
    "### 4.2.3 Random Forest\n",
    " \n",
    "**Approach:**\n",
    "The random forest regression was hired, hoping to have a better performance to predict the weekly rental price for each suburb. Since random forest regression also contains a feature importance function that shows how each predictor contributes to the prediction of our target class, our team used random forest regression. The strength of random forest regressor that can be trained on both categorical and continuous values, was another reason to choose to use this model as we have 2 categorical variables, residence type and SA2 codes. <br/>\n",
    "However, since we are not entirely sure if the random forest regression will work fine with our dataset, we still had to test the model before training the model with the dataset. To see the testings on random forest regression in terms of testing and training accuracies, please refer to `/models/randomForest.ipynb`. To see the implementation of the random forest regression and its predictions, please refer to `/models/pred_w_random_forest.ipynb`. \n",
    " \n",
    "**Features used:** All features were selected after the forward selection using MSE from a random forest tree. By referring to the overview of the features, all the internal features and the distance features from the external are assumed not to be changed in the next 5 years (2023 - 2027). The other features which we call them by ‘economics factors’ (GDP, saving rate, income per person, crime case, and population density) were predicted with linear and quadratic regressions and have corresponding values for each future year (2023, 2024, 2025, 2026, and 2027). Henceforth, we need to have all economic features because otherwise, without these economic features the prediction input dataset will have the same feature data for all the next five years. Therefore, we decided to put only the internal predictors and distance predictors from the external into the forward feature selection with the lowest MSEs. To have small MSE (mean square error) values can relate to a better performance of the prediction model in terms of accuracy.\n",
    "After running the forward selection with MSEs, we got all the internal and distance predictors which mean all the features are needed to predict weekly rental price with random forest regressor. \n",
    "\n",
    "**Findings:** We have predicted the weekly rental price for each property in each district throughout 2023 - 2027. They are organised in csv format with columns of year of prediction, SA2 code of a rental property, predicted weekly rental price of a rental property. These data will be used to compute an average growth rate for the future years. \n",
    "\n",
    "**Any issues you ran into:** In order to convert categorical data into numerical data, we first used get_dummies function but by doing so, the columns (features) for training and prediction dataset were different which was hard to manage and had to choose loss of some data due to this by dropping the columns that are not showing up both training and prediction datasets. This happened mainly due to SA2 codes where a few of them were missing in the prediction dataset and the main characteristic of random forest regressor that requires the exact same features for both training and prediction stages. This issue has been handled by adopting a different converting method, labelEncoding().\n",
    " \n",
    "**Assumption & Limitation:**\n",
    "It is erased from `/models/randomForest.ipynb` notebook, but the feature selection was implemented by recursive feature elimination (RFE) because the random forest regressor only provides feature importances which leave us to select features out of them by ourselves. However, RFE method was deleted since it takes an extremely long time and could not be done with our current devices, where the kernel gets killed in the middle of running the feature selection. Therefore, due to the limitation of hardwares, we implemented forward selection instead. \n",
    " \n",
    " \n",
    "### 4.3 Rental Price Growth Rate Calculation\n",
    " \n",
    "**Approach:**\n",
    "Based on prediction results from 2023-2027, rental prices are aggregated to average per SA2 district per year. The same aggregation is applied to the year 2022 rental dataset, which is then concatenated to the prediction set. Growth rate of a district in the current year is calculated by (average rental price of district in current year)-(average rental price of district in past year) / (average rental price of district in past year). The average future growth rate for a district is then calculated by (sum of yearly growth rates from 2023 to 2027 of the district) / (5). At the end of this notebook, it returns a csv file such as `/data/curated/final_growing_rates_rf.csv` that shows the average growing rate predicted for the next 5 years of each suburb, sorted in an descending order of average growing rate.\n",
    "To view these steps please see `/models/growthRateCalc.ipynb`.\n",
    " \n",
    "`/notebooks/3.growth_rate_geo.ipynb` presents a geo visualisation of rent growth rate calculated by random forest model and xgboost model.\n",
    " \n",
    "**Assumption and limitation:**\n",
    "The avearged growth rate per sa2 suburb could be dependent on the number of properties in that suburb. Hence high grow rates might be predicted for a suburb with few properties in a sa2 suburb.\n",
    "A high nominal average of the next five year growth rates might not directly indicate rental prosperity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Liveability Scoring and Ranking Algorithm\n",
    "`notebooks/Liveability/scoring.ipynb`: \n",
    "**Approach:** This notebook developed a ranking system for liveability scoring (0-100). The notebook firstly checked the distribution of each feature to see whether standardisation can be applied. Unfortunately, data were not normally distributed. Hence we used score = rank / len(df) to perform the score for each livability criteria and sum them with weight specified by our user.\n",
    " \n",
    "**Assumption & Limitation:** We assume that the livability score is subjective and individuals care more about the livability based on their own standards. For limitation, we found that the distribution of the distance features were not normal, hence we didn’t standardise the distances. Instead, we used ranking to add up a score between 0 and 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Website Building\n",
    " \n",
    "**Approach:**\n",
    " \n",
    "To put our model into practice and bridge the gap between the general population and our models, we built a web site that enables the general population to explore our models.\n",
    " \n",
    "`/web/app.py` is an app object created using the Flask class. We have rendered our models including population density prediction, crime case prediction, income rate prediction, and rental price prediction models on our website. We also created a client-oriented section for our users to get recommendations on the mostly liveable and affordable suburbs and properties based on their needs. Users can input the year and SA2 code in order to view the corresponding prediction result by our models.\n",
    " \n",
    "We also included Suburb Name and SA2 code lookup tables for our users to get the SA2 code by suburb name. The html lookup table was transferred from the csv lookup table. The transferring process is included at the end of `/notebooks/preliminary_property.ipynb`.\n",
    " \n",
    "To view the models, please visit the corresponding notebooks in `/models`. At the end of each notebook, the model will be saved with `.pkl` format for the use of Flask. The `.pkl` model files are saved under `/web/models`.\n",
    " \n",
    "To view the website, please run `/web/app.py` and go to `http://127.0.0.1:5000/ ` or you can directly visit https://smart-renting-vic.herokuapp.com/ \n",
    " \n",
    "To view the css styles of the website, please see`/web/static/style1.css` .\n",
    " \n",
    "To view the html code of the website, please see `web/templates/index.html`.\n",
    " \n",
    "**Assumption & Limitation:** Limitation: Our initial github repo is too big and it can’t be deployed directly with heroku. Hence, we extracted the files we used for the website to a separate repo and deployed that repo.\n",
    " \n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
