{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Links for Historical Rental Records for all Postcodes\n",
    "## DO NOT RUN ALL, READ ALL CELLS FIRST\n",
    "This notebook will get all links of rental records per postcode from: https://www.oldlistings.com.au/site-map?page=17&state=VIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from requests import get\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "from random import seed\n",
    "from random import random\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no need to run this, but don't delete the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    " \n",
    "page = 18 \n",
    "urls = []\n",
    "for i in range(page):\n",
    "    webpage = f'https://www.oldlistings.com.au/site-map?page={i}&state=VIC'\n",
    "    reqs = requests.get(webpage)\n",
    "    soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href')\n",
    "        if 'rent' in href:\n",
    "            url = 'https://www.oldlistings.com.au'+href\n",
    "            print(url)\n",
    "            urls.append(url)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get rental urls per postcode with regular expression\n",
    "### website list saved (no need to run this, but don't delete the following cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5388"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "regex = r'https:/[/]www.oldlistings.com.au[/]\\w+[-]\\w+[/]VIC[/]\\w+[+]?\\w+[/]\\d+[/]rent[/]\\d?$'\n",
    "rent_urls = []\n",
    "for url in urls:\n",
    "    rent_url = re.findall(regex, url)\n",
    "    rent_urls.append(rent_url)\n",
    "rent_urls\n",
    "flat_list = [item for sublist in rent_urls for item in sublist]\n",
    "flat_list\n",
    "\n",
    "\n",
    "l = list(set(flat_list))\n",
    "l.sort()\n",
    "with open(\"HIS_url_links\", \"wb\") as fp:\n",
    "    pickle.dump(l, fp)\n",
    "len(l)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "newpath = r'../../data/raw/historical_data' \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_daily(start, end):\n",
    "    with open(\"HIS_url_links\", \"rb\") as fp:\n",
    "        result_urls = pickle.load(fp)\n",
    "    result_urls_partial = result_urls[start:end]\n",
    "    houses = []\n",
    "    posts = []\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Mobile Safari/537.36\"}\n",
    "\n",
    "    for i in range(len(result_urls_partial)):\n",
    "        soup = BeautifulSoup(requests.get(result_urls_partial[i], headers=headers).text, 'html.parser')\n",
    "        # in the html of the page, find all the bins with <li> and class:\n",
    "        print(f'{i}.getting data from {result_urls_partial[i]}')\n",
    "        house_data = soup.find_all(\"div\", {\"class\":re.compile(\"property (odd|even) clearfix\")})\n",
    "        regex = r'(\\d{4})'\n",
    "        post = re.findall(regex, result_urls_partial[i])\n",
    "        # random wait times\n",
    "        value = random()\n",
    "        scaled_value = 1 + (value * (9 - 5))\n",
    "        # print(scaled_value)\n",
    "        time.sleep(scaled_value)\n",
    "\n",
    "        # no need this\n",
    "        #for link in result_urls_partial:\n",
    "        #    post = re.findall(regex, link)\n",
    "        #    postcode = post[0][0:4]\n",
    "\n",
    "        houses.extend(house_data)\n",
    "        for c in range(len(house_data)):\n",
    "            posts.append(post)\n",
    "        \n",
    "        \n",
    "    count = 0\n",
    "    data = pd.DataFrame()\n",
    "    first = True\n",
    "\n",
    "    count_p = 0\n",
    "    for num in houses:\n",
    "        # getting the price: make sure to test this code a few times by itself to understand exactly which parameters will work \n",
    "        current_priceTag = num.find_all('section', {\"class\":\"grid-35 tablet-grid-35 price\"})\n",
    "        priceSection = num.find_all('section',{\"class\":\"grid-100 historical-price\"})\n",
    "        priceTags = []\n",
    "        rent_prices = []\n",
    "        rent_dates = []\n",
    "\n",
    "        try:\n",
    "            current_price = re.search(r'\\<h3\\>(.*)\\<\\/h3\\>', str(current_priceTag))\n",
    "            current_date = re.search(r'\\:[ ](.*)\\<\\/span\\>', str(current_priceTag))\n",
    "            rent_prices.append(current_price.group(1))\n",
    "            rent_dates.append(current_date.group(1))\n",
    "        except:\n",
    "            print(\"Current rent value exception\")\n",
    "\n",
    "        priceTags = priceSection[0].find_all('li')\n",
    "        for ps in priceTags:\n",
    "            try:\n",
    "                rent = re.search(r'\\<\\/span\\>(.*)\\<\\/li\\>', str(ps))\n",
    "                rent_prices.append(rent.group(1))\n",
    "                date = re.search(r'\\<span\\>(.*)\\<\\/span\\>', str(ps))\n",
    "                rent_dates.append(date.group(1))\n",
    "            except:\n",
    "                print(\"Historical values exception\")\n",
    "\n",
    "        try:\n",
    "            lat = re.search(r'data\\-lat\\=\\\"(.*\\d)\\\"[ ]', str(num)) \n",
    "            lng = re.search(r'data\\-lng\\=\\\"(.*\\d)\\\"\\>', str(num)) \n",
    "            latitude = lat.group(1)\n",
    "            longitude = lng.group(1)\n",
    "        except:\n",
    "            print(\"Location values exception\")\n",
    "\n",
    "        try:\n",
    "            nbed = num.find_all('p', {\"class\": \"property-meta bed\"})[0].text\n",
    "            nbed = re.search(r'[A-Za-z][ ]:[ ](.*)', nbed).group(1)\n",
    "        except IndexError:\n",
    "            nbed = 'none'\n",
    "        try:\n",
    "            nbath = num.find_all('p', {\"class\": \"property-meta bath\"})[0].text\n",
    "            nbath = re.search(r'[A-Za-z][ ]:[ ](.*)', nbath).group(1)\n",
    "        except IndexError:\n",
    "            nbath = 'none'\n",
    "        try:\n",
    "            ncar = num.find_all('p', {\"class\": \"property-meta car\"})[0].text\n",
    "            ncar = re.search(r'[A-Za-z][ ]:[ ](.*)', ncar).group(1)\n",
    "        except IndexError:\n",
    "            ncar = 'none'\n",
    "        try:\n",
    "            type = num.find_all('p', {\"class\": \"property-meta type\"})[0].text\n",
    "            type = re.search(r'[A-Za-z][ ]:[ ](.*)', type).group(1)\n",
    "        except IndexError:\n",
    "            type = 'none'\n",
    "        try:\n",
    "            address = num.find_all('h2', {\"class\":\"address\"})[0].text\n",
    "        except IndexError:\n",
    "            address = 'none'\n",
    "\n",
    "        for z in zip(rent_prices, rent_dates):\n",
    "            d = {\"address\":[address], \"latitude\":[latitude], \"longitude\":[longitude], \n",
    "                \"nbed\":[nbed], \"nbath\":[nbath], \"ncar\":[ncar], \"historical_prices\":[z[0]], \n",
    "                \"type\":[type],\n",
    "                \"historical_dates\":[z[1]],\n",
    "                \"postcode\":posts[count_p]}\n",
    "\n",
    "            if first:\n",
    "                first = False\n",
    "                data = pd.DataFrame.from_dict(d)\n",
    "            else:\n",
    "                data = pd.concat([data, pd.DataFrame.from_dict(d)])\n",
    "\n",
    "        count_p += 1\n",
    "    print(data.head(10))\n",
    "    data.to_csv(f'../../data/raw/historical_data/{start}_{end}_historical.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exaple demonstration: calling function \n",
    "### 100 websites will be scraped per call, will retrieve round 10k data each call\n",
    "### no need to run the following cells (can view the output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: call the first 100 urls.\n",
    "# get_urls_daily(0,5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: call the 100-200 urls. (if you wish to run more than 200, CHANGE to another wifi and call)\n",
    "# get_urls_daily(100, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What the team needs to do (Write and Run from here)\n",
    "### Call the function at least twice a day (just to be safe, only make 2 calls with 1 wifi)\n",
    "following is an example (do not call yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**07/09/2022 Wed**\n",
    "*   grace: 0-200\n",
    "\n",
    "*   jin: 200-400\n",
    "\n",
    "*   john: 400-600\n",
    "\n",
    "*   katherine: 600-800\n",
    "\n",
    "*   philip: 800-1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grace\n",
    "get_urls_daily(0, 100) # first call \n",
    "get_urls_daily(100, 200) # second call "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jin\n",
    "get_urls_daily(100, 200)\n",
    "get_urls_daily(200, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comment out the function after calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**08/09/2022 Thur** (this schedule may be changed)\n",
    "*   grace: 1000-1200\n",
    "\n",
    "*   jin: 1200-1400\n",
    "\n",
    "*   john: 1400-1600\n",
    "\n",
    "*   katherine: 1600-1800\n",
    "\n",
    "*   philip: 1800-2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grace\n",
    "get_urls_daily(1000, 1100)\n",
    "get_urls_daily(1100, 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
