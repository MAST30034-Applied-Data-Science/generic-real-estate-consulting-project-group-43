{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this notebook merges values of external features to the dataset\n",
    "\n",
    "Time period of founded data \n",
    "income per person: 2006 - 2019\n",
    "population density: 2001-2021\n",
    "GDP: 1960 - 2020\n",
    "Saving rate: 1970 - 2020\n",
    "crime cases: 2013 - 2022\n",
    "\n",
    "Since we plan to train models for 2013 - 2022, those missing values of external features (i.e. 2020 - 2022 of income per person) will be filled up with predicted features' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create path\n",
    "new_path = '../../data/curated/merged_dataset/'\n",
    "\n",
    "if not os.path.exists(new_path):\n",
    "    os.makedirs(new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge GDP and saving rate and drop month to get values of all attributes based on year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gdp and saving rate data and predicted gdp and saving rate data\n",
    "gdp_sr = pd.read_csv('../../data/curated/gdp_with_saving_rate.csv')\n",
    "pred_gdp_sr = pd.read_csv('../../data/curated/feature_prediction/21_27_gdp_with_saving_predicted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]\n",
    "\n",
    "for i in year:\n",
    "    if i not in [2021, 2022]:\n",
    "        data = pd.read_csv(f'../../data/curated/min_distance_sa2_organised/{i}_property_with_SA2.csv', index_col=[0])\n",
    "\n",
    "        # fill up missing distance data with maximum distance\n",
    "        max_cbd = np.nanmax(data['min_distance_to_cbd'].values)\n",
    "        max_park = np.nanmax(data['min_distance_to_park'].values)\n",
    "        max_prim = np.nanmax(data['min_distance_to_prim'].values)\n",
    "        max_second = np.nanmax(data['min_distance_to_second'].values)\n",
    "        max_train = np.nanmax(data['min_distance_to_train'].values)\n",
    "        max_poli = np.nanmax(data['min_distance_to_poli'].values)\n",
    "        max_hosp = np.nanmax(data['min_distance_to_hosp'].values)\n",
    "        max_shop = np.nanmax(data['min_distance_to_shop'].values)\n",
    "\n",
    "        data = data.fillna({'min_distance_to_cbd': max_cbd, 'min_distance_to_park': max_park, 'min_distance_to_prim': max_prim, 'min_distance_to_second': max_second, \n",
    "        'min_distance_to_train': max_train, 'min_distance_to_hosp': max_hosp, 'min_distance_to_poli': max_poli, 'min_distance_to_shop': max_shop})    \n",
    "        \n",
    "        # drop months to get values of all attributes based on year\n",
    "        data = data.groupby(['year', 'address', 'latitude', 'longitude', 'postcode',\n",
    "            'sa2_2021', 'sa2_2016', 'residence_type', 'nbed', 'nbath', 'ncar'], as_index=False)\\\n",
    "        .agg({'min_distance_to_cbd': 'first', 'min_distance_to_park': 'first', 'min_distance_to_prim': 'first', 'min_distance_to_second': 'first', 'min_distance_to_train': 'first', \n",
    "        'min_distance_to_hosp': 'first', 'min_distance_to_poli': 'first', 'min_distance_to_shop': 'first', 'weekly_rent': 'mean'})\n",
    "\n",
    "        # merge gdp and saving rate data with dataset of organised dataset (except 2021 and 2022) based on year\n",
    "        gdp = list(gdp_sr.loc[gdp_sr['year'] == i, 'gdp(USD Millioins)'])[0]\n",
    "        sr = list(gdp_sr.loc[gdp_sr['year'] == i, 'saving_rate(% of GDP)'])[0]\n",
    "        \n",
    "        data[list(gdp_sr.columns)[2]] = gdp\n",
    "        data[list(gdp_sr.columns)[3]] = sr\n",
    "\n",
    "    else:\n",
    "        data = pd.read_csv(f'../../data/curated/min_distance_sa2_organised/{i}_property_with_SA2.csv', index_col=[0])\n",
    "\n",
    "        # fill up missing distance data with maximum distance\n",
    "        max_cbd = np.nanmax(data['min_distance_to_cbd'].values)\n",
    "        max_park = np.nanmax(data['min_distance_to_park'].values)\n",
    "        max_prim = np.nanmax(data['min_distance_to_prim'].values)\n",
    "        max_second = np.nanmax(data['min_distance_to_second'].values)\n",
    "        max_train = np.nanmax(data['min_distance_to_train'].values)\n",
    "        max_poli = np.nanmax(data['min_distance_to_poli'].values)\n",
    "        max_hosp = np.nanmax(data['min_distance_to_hosp'].values)\n",
    "        max_shop = np.nanmax(data['min_distance_to_shop'].values)\n",
    "\n",
    "        data = data.fillna({'min_distance_to_cbd': max_cbd, 'min_distance_to_park': max_park, 'min_distance_to_prim': max_prim, 'min_distance_to_second': max_second, \n",
    "        'min_distance_to_train': max_train, 'min_distance_to_hosp': max_hosp, 'min_distance_to_poli': max_poli, 'min_distance_to_shop': max_shop})   \n",
    "        \n",
    "        # drop months to get values of all attributes based on year\n",
    "        data = data.groupby(['year', 'address', 'latitude', 'longitude', 'postcode',\n",
    "            'sa2_2021', 'sa2_2016', 'residence_type', 'nbed', 'nbath', 'ncar'], as_index=False)\\\n",
    "        .agg({'min_distance_to_cbd': 'first', 'min_distance_to_park': 'first', 'min_distance_to_prim': 'first', 'min_distance_to_second': 'first', 'min_distance_to_train': 'first', \n",
    "        'min_distance_to_hosp': 'first', 'min_distance_to_poli': 'first', 'min_distance_to_shop': 'first', 'weekly_rent': 'mean'})\n",
    "\n",
    "        # merge predicted gdp and saving rate data with dataset of organised dataset (2021 and 2022) based on year\n",
    "        gdp = list(pred_gdp_sr.loc[pred_gdp_sr['year'] == i, 'gdp'])[0]\n",
    "        sr = list(pred_gdp_sr.loc[pred_gdp_sr['year'] == i, 'saving'])[0]\n",
    "        \n",
    "        data[list(gdp_sr.columns)[2]] = gdp\n",
    "        data[list(gdp_sr.columns)[3]] = sr\n",
    "\n",
    "    # output csv file of merged dataset\n",
    "    data.to_csv(f'../../data/curated/merged_dataset/{i}_merged_data.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge income per person for each sa2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load income per person for each sa2 data and predicted income per person for each sa2 data\n",
    "income = pd.read_csv('../../data/curated/income_per_person_sa2.csv', index_col=[0])\n",
    "income = income.rename(columns={'SA2': 'sa2_2016'})\n",
    "pred_income = pd.read_csv('../../data/curated/feature_prediction/20_27_income_per_person_2016sa2.csv', index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]\n",
    "\n",
    "for i in year:\n",
    "    if i not in [2020, 2021, 2022]:\n",
    "        data = pd.read_csv(f'../../data/curated/merged_dataset/{i}_merged_data.csv', index_col=[0])\n",
    "\n",
    "        # merge income per person for each sa2 with previous merged dataset (except 2020, 2021 and 2022) based on 2016 sa2\n",
    "        income_data = income.loc[:, ['sa2_2016', str(i)]]\n",
    "\n",
    "        data['income_per_person'] = (data.merge(income_data, on=['sa2_2016'], how='left')[str(i)])\n",
    "\n",
    "    else:\n",
    "        data = pd.read_csv(f'../../data/curated/merged_dataset/{i}_merged_data.csv', index_col=[0])\n",
    "\n",
    "        # merge predicted income per person for each sa2 with previous merged dataset (2020, 2021 and 2022) based on 2016 sa2\n",
    "        income_data = pred_income.loc[pred_income['Year'] == i][['sa2_2016', 'income_per_person_sa2']]\n",
    "\n",
    "        data['income_per_person'] = (data.merge(income_data, on=['sa2_2016'], how='left')['income_per_person_sa2'])\n",
    "\n",
    "    # output csv file of merged dataset\n",
    "    data.to_csv(f'../../data/curated/merged_dataset/{i}_merged_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge population density for each sa2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load population density for each sa2 data and predicted population density for each sa2 data\n",
    "population = pd.read_csv('../../data/curated/vic_population_density_sa2.csv', index_col=[0])\n",
    "population = population.rename(columns={'SA2 code': 'sa2_2021'})\n",
    "pred_population = pd.read_csv('../../data/curated/feature_prediction/22_27_population.csv', index_col=[0])\n",
    "pred_population = pred_population.rename(columns={'SA2 code': 'sa2_2021'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]\n",
    "\n",
    "for i in year:\n",
    "    if i != 2022:\n",
    "        data = pd.read_csv(f'../../data/curated/merged_dataset/{i}_merged_data.csv', index_col=[0])\n",
    "\n",
    "        # merge population density for each sa2 with previous merged dataset (except 2022) based on 2021 sa2\n",
    "        population_data = population.loc[:, ['sa2_2021', f'population_density_of_{str(i)}']]\n",
    "\n",
    "        data['population_density'] = (data.merge(population_data, on=['sa2_2021'], how='left')[f'population_density_of_{str(i)}'])\n",
    "\n",
    "    else:\n",
    "        data = pd.read_csv(f'../../data/curated/merged_dataset/{i}_merged_data.csv', index_col=[0])\n",
    "        \n",
    "        # merge predicted population density for each sa2 with previous merged dataset (2022) based on 2021 sa2\n",
    "        population_data = pred_population.loc[pred_population['year'] == i][['sa2_2021', 'pred']]\n",
    "\n",
    "        data['population_density'] = (data.merge(population_data, on=['sa2_2021'], how='left')['pred'])\n",
    "\n",
    "    # output csv file of merged dataset\n",
    "    data.to_csv(f'../../data/curated/merged_dataset/{i}_merged_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge crime cases by postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load crime cases with postcode data\n",
    "crime_cases = pd.read_csv('../../data/curated/crime_cases.csv', index_col=[0])\n",
    "crime_cases = crime_cases.rename(columns={'Postcode': 'postcode'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]\n",
    "\n",
    "for i in year:\n",
    "    data = pd.read_csv(f'../../data/curated/merged_dataset/{i}_merged_data.csv', index_col=[0])\n",
    "\n",
    "    # merge crime cases with previous merged dataset (except 2022) based on postcode\n",
    "    crime_cases_data = crime_cases.loc[crime_cases['Year'] == i][['postcode', 'Offence Count']]\n",
    "\n",
    "    data['crime_cases'] = (data.merge(crime_cases_data, on=['postcode'], how='left')['Offence Count'])\n",
    "\n",
    "    # output csv file of merged dataset\n",
    "    data.to_csv(f'../../data/curated/merged_dataset/{i}_merged_data.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
