{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "\n",
    "def add_sa2(data, sf, name):\n",
    "    data = data.dropna(subset=['longitude', 'latitude'])\n",
    "    #if \n",
    "    data['SA2_CODE'] = np.nan\n",
    "    data = data.astype({'longitude': 'float', 'latitude': 'float'})\n",
    "\n",
    "    print(\"Estimated processing size:\", len(sf) * len(data))\n",
    "\n",
    "    # Allocate SA2 code based on the coordinates from rent data\n",
    "    for index, row in data.iterrows():\n",
    "        for index_area, row_area in sf.iterrows():\n",
    "            geo = row_area[\"geometry\"]\n",
    "            fit = False         \n",
    "            fit = geo.contains(Point(row[\"longitude\"], row[\"latitude\"]))\n",
    "\n",
    "            if fit:\n",
    "                data.loc[index,'SA2_CODE'] = row_area[\"SA2_CODE21\"]\n",
    "                # print(row['id'], \":\", row_area[\"SA2_CODE21\"])\n",
    "                break\n",
    "\n",
    "    # Show data loss\n",
    "    len_data = len(data)\n",
    "    len_result = len(data.dropna(subset=['SA2_CODE']))\n",
    "    print(\"Original size:\", len_data, \"=> Result size:\", len_result)\n",
    "    print(\"Loss:\", len_data - len_result)\n",
    "\n",
    "    # Convert SA2 code as Interger\n",
    "    data = data.dropna(subset=['SA2_CODE'])\n",
    "    data['SA2_CODE'] = data[\"SA2_CODE\"].astype(int)\n",
    "\n",
    "    # Export as csv\n",
    "    data.to_csv(\"../../data/curated/property_all_with_SA2/\"+name+\"_property_with_SA2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read SA2-Geolocation data (shape file)\n",
    "# https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files\n",
    "sf = gpd.read_file(\"../../data/raw/Geo/SA2_2021_AUST_SHP_GDA2020/SA2_2021_AUST_GDA2020.shp\")\n",
    "\n",
    "# Slice the geolocation for Victoria\n",
    "COL_SF = [\"SA2_CODE21\", \"geometry\"]\n",
    "sf = sf.loc[sf[\"STE_CODE21\"] == '2']\n",
    "sf['geometry'] = sf['geometry'].to_crs(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\")\n",
    "sf = sf[COL_SF]\n",
    "sf = sf.dropna(subset=['geometry'])\n",
    "sf[\"SA2_CODE21\"] = sf[\"SA2_CODE21\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../data/curated/property_all_no_outlier/2006_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2007_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2008_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2009_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2010_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2011_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2012_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2013_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2014_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2015_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2016_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2017_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2018_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2019_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2020_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2021_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2022_property_no_outlier.csv']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "path = \"../../data/curated/property_all_no_outlier/*.csv\"\n",
    "property_all_lst = []\n",
    "for fname in glob.glob(path):\n",
    "    property_all_lst.append(fname)\n",
    "property_all_lst = sorted(property_all_lst)\n",
    "property_all_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006\n",
      "Estimated processing size: 267264\n",
      "Original size: 512 => Result size: 512\n",
      "Loss: 0\n",
      "2007\n",
      "Estimated processing size: 2418426\n",
      "Original size: 4633 => Result size: 4632\n",
      "Loss: 1\n",
      "2008\n",
      "Estimated processing size: 2816712\n",
      "Original size: 5396 => Result size: 5376\n",
      "Loss: 20\n",
      "2009\n",
      "Estimated processing size: 2650716\n",
      "Original size: 5078 => Result size: 5058\n",
      "Loss: 20\n",
      "2010\n",
      "Estimated processing size: 3201426\n",
      "Original size: 6133 => Result size: 6081\n",
      "Loss: 52\n",
      "2011\n",
      "Estimated processing size: 4530960\n",
      "Original size: 8680 => Result size: 8558\n",
      "Loss: 122\n",
      "2012\n",
      "Estimated processing size: 5698674\n",
      "Original size: 10917 => Result size: 10830\n",
      "Loss: 87\n",
      "2013\n",
      "Estimated processing size: 6039018\n",
      "Original size: 11569 => Result size: 11466\n",
      "Loss: 103\n",
      "2014\n",
      "Estimated processing size: 6544836\n",
      "Original size: 12538 => Result size: 12489\n",
      "Loss: 49\n",
      "2015\n",
      "Estimated processing size: 6748416\n",
      "Original size: 12928 => Result size: 12782\n",
      "Loss: 146\n",
      "2016\n",
      "Estimated processing size: 7968852\n",
      "Original size: 15266 => Result size: 15119\n",
      "Loss: 147\n",
      "2017\n",
      "Estimated processing size: 9046782\n",
      "Original size: 17331 => Result size: 17143\n",
      "Loss: 188\n",
      "2018\n",
      "Estimated processing size: 10375794\n",
      "Original size: 19877 => Result size: 19604\n",
      "Loss: 273\n",
      "2019\n",
      "Estimated processing size: 11406222\n",
      "Original size: 21851 => Result size: 21554\n",
      "Loss: 297\n",
      "2020\n",
      "Estimated processing size: 11477214\n",
      "Original size: 21987 => Result size: 21638\n",
      "Loss: 349\n",
      "2021\n",
      "Estimated processing size: 14112270\n",
      "Original size: 27035 => Result size: 26595\n",
      "Loss: 440\n",
      "2022\n",
      "Estimated processing size: 38439558\n",
      "Original size: 73639 => Result size: 73107\n",
      "Loss: 532\n"
     ]
    }
   ],
   "source": [
    "for path in property_all_lst:\n",
    "    \n",
    "    regex = r'\\d+\\w\\d+'\n",
    "    year = re.findall(regex, path)[0]\n",
    "    print(year)\n",
    "    # Read Rent Data\n",
    "    data = pd.read_csv(path)\n",
    "    #print(data.longitude.sum())\n",
    "    add_sa2(data, sf, year)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "no_outlier_path = '../../data/curated/property_all_no_outlier'\n",
    "shutil.rmtree(no_outlier_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import folium\n",
    "# import numpy as np\n",
    "# \n",
    "# # make geometry as JSON type\n",
    "# geoJSON = sf['geometry'].to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Map whole SA2 area\n",
    "# _map = folium.Map(location=[-37, 144], tiles=\"Stamen Terrain\", zoom_start=10)\n",
    "# \n",
    "# _map.add_child(folium.Choropleth(\n",
    "#     geo_data=geoJSON,\n",
    "#     name='SA2 Area',\n",
    "# ))\n",
    "# \n",
    "# _map.save('../../plots/SA2_Map.html')\n",
    "# _map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Point rent data in the map\n",
    "# \n",
    "# data_s = data.dropna(subset=['latitude', 'longitude'])\n",
    "# \n",
    "# id_data = data_s['id']\n",
    "# latitude_data = data_s['latitude']\n",
    "# longitude_data = data_s['longitude']\n",
    "# \n",
    "# # plot points out of geolocation\n",
    "# for id, lati, long in zip(id_data, latitude_data, longitude_data):\n",
    "#     _map.add_child(\n",
    "#         folium.Marker(location=[lati, long], popup=str(id))\n",
    "#     )\n",
    "# \n",
    "# _map.save('../../plots/rentalData_in_SA2Location.html')\n",
    "# _map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit ('3.10.5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ddc0034ad7dea0900e87274559c7f730dcb76ce48c43b93b61df066c264fb9ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
