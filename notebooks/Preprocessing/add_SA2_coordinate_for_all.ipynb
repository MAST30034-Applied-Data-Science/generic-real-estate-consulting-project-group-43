{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "import os\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_sa2(data, sf, name):\n",
    "    data = data.dropna(subset=['longitude', 'latitude'])\n",
    "    #if \n",
    "    data['SA2_CODE'] = np.nan\n",
    "    data = data.astype({'longitude': 'float', 'latitude': 'float'})\n",
    "\n",
    "    print(\"Estimated processing size:\", len(sf) * len(data))\n",
    "\n",
    "    # Allocate SA2 code based on the coordinates from rent data\n",
    "    for index, row in data.iterrows():\n",
    "        for index_area, row_area in sf.iterrows():\n",
    "            geo = row_area[\"geometry\"]\n",
    "            fit = False         \n",
    "            fit = geo.contains(Point(row[\"longitude\"], row[\"latitude\"]))\n",
    "\n",
    "            if fit:\n",
    "                data.loc[index,'SA2_CODE'] = row_area[\"SA2_CODE21\"]\n",
    "                # print(row['id'], \":\", row_area[\"SA2_CODE21\"])\n",
    "                break\n",
    "\n",
    "    # Show data loss\n",
    "    len_data = len(data)\n",
    "    len_result = len(data.dropna(subset=['SA2_CODE']))\n",
    "    print(\"Original size:\", len_data, \"=> Result size:\", len_result)\n",
    "    print(\"Loss:\", len_data - len_result)\n",
    "\n",
    "    # Convert SA2 code as Interger\n",
    "    data = data.dropna(subset=['SA2_CODE'])\n",
    "    data['SA2_CODE'] = data[\"SA2_CODE\"].astype(int)\n",
    "\n",
    "    # Export as csv\n",
    "    data.to_csv(\"../../data/curated/property_all_with_SA2/\"+name+\"_property_with_SA2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read SA2-Geolocation data (shape file)\n",
    "# https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files\n",
    "sf = gpd.read_file(\"../../data/raw/Geo/SA2_2021_AUST_SHP_GDA2020/SA2_2021_AUST_GDA2020.shp\")\n",
    "\n",
    "# Slice the geolocation for Victoria\n",
    "COL_SF = [\"SA2_CODE21\", \"geometry\"]\n",
    "sf = sf.loc[sf[\"STE_CODE21\"] == '2']\n",
    "sf['geometry'] = sf['geometry'].to_crs(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\")\n",
    "sf = sf[COL_SF]\n",
    "sf = sf.dropna(subset=['geometry'])\n",
    "sf[\"SA2_CODE21\"] = sf[\"SA2_CODE21\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../data/curated/property_all_no_outlier/2006_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2007_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2008_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2009_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2010_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2011_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2012_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2013_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2014_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2015_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2016_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2017_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2018_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2019_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2020_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2021_property_no_outlier.csv',\n",
       " '../../data/curated/property_all_no_outlier/2022_property_no_outlier.csv']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "path = \"../../data/curated/property_all_no_outlier/*.csv\"\n",
    "property_all_lst = []\n",
    "for fname in glob.glob(path):\n",
    "    property_all_lst.append(fname)\n",
    "property_all_lst = sorted(property_all_lst)\n",
    "property_all_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006\n",
      "Estimated processing size: 267264\n",
      "Original size: 512 => Result size: 512\n",
      "Loss: 0\n",
      "2007\n",
      "Estimated processing size: 2418426\n",
      "Original size: 4633 => Result size: 4632\n",
      "Loss: 1\n",
      "2008\n",
      "Estimated processing size: 2816712\n",
      "Original size: 5396 => Result size: 5376\n",
      "Loss: 20\n",
      "2009\n",
      "Estimated processing size: 2650716\n",
      "Original size: 5078 => Result size: 5058\n",
      "Loss: 20\n",
      "2010\n",
      "Estimated processing size: 3201426\n",
      "Original size: 6133 => Result size: 6081\n",
      "Loss: 52\n",
      "2011\n",
      "Estimated processing size: 4530960\n",
      "Original size: 8680 => Result size: 8558\n",
      "Loss: 122\n",
      "2012\n",
      "Estimated processing size: 5698674\n",
      "Original size: 10917 => Result size: 10830\n",
      "Loss: 87\n",
      "2013\n",
      "Estimated processing size: 6039018\n",
      "Original size: 11569 => Result size: 11466\n",
      "Loss: 103\n",
      "2014\n",
      "Estimated processing size: 6544836\n",
      "Original size: 12538 => Result size: 12489\n",
      "Loss: 49\n",
      "2015\n",
      "Estimated processing size: 6748416\n",
      "Original size: 12928 => Result size: 12782\n",
      "Loss: 146\n",
      "2016\n",
      "Estimated processing size: 7968852\n",
      "Original size: 15266 => Result size: 15119\n",
      "Loss: 147\n",
      "2017\n",
      "Estimated processing size: 9046782\n",
      "Original size: 17331 => Result size: 17143\n",
      "Loss: 188\n",
      "2018\n",
      "Estimated processing size: 10375794\n",
      "Original size: 19877 => Result size: 19604\n",
      "Loss: 273\n",
      "2019\n",
      "Estimated processing size: 11406222\n",
      "Original size: 21851 => Result size: 21554\n",
      "Loss: 297\n",
      "2020\n",
      "Estimated processing size: 11477214\n",
      "Original size: 21987 => Result size: 21638\n",
      "Loss: 349\n",
      "2021\n",
      "Estimated processing size: 14112270\n",
      "Original size: 27035 => Result size: 26595\n",
      "Loss: 440\n",
      "2022\n",
      "Estimated processing size: 38439558\n",
      "Original size: 73639 => Result size: 73107\n",
      "Loss: 532\n"
     ]
    }
   ],
   "source": [
    "for path in property_all_lst:\n",
    "    \n",
    "    regex = r'\\d+\\w\\d+'\n",
    "    year = re.findall(regex, path)[0]\n",
    "    print(year)\n",
    "    # Read Rent Data\n",
    "    data = pd.read_csv(path)\n",
    "    #print(data.longitude.sum())\n",
    "    add_sa2(data, sf, year)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "no_outlier_path = '../../data/curated/property_all_no_outlier'\n",
    "shutil.rmtree(no_outlier_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding SA2 For Facilities (School, Park, Train Station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../data/curated/features_of_interst/park/park_2013.csv',\n",
       " '../../data/curated/features_of_interst/park/park_2014.csv',\n",
       " '../../data/curated/features_of_interst/park/park_2015.csv',\n",
       " '../../data/curated/features_of_interst/park/park_2016.csv',\n",
       " '../../data/curated/features_of_interst/park/park_2017.csv',\n",
       " '../../data/curated/features_of_interst/park/park_2018.csv',\n",
       " '../../data/curated/features_of_interst/park/park_2019.csv',\n",
       " '../../data/curated/features_of_interst/park/park_2020.csv',\n",
       " '../../data/curated/features_of_interst/park/park_2021.csv',\n",
       " '../../data/curated/features_of_interst/primary/primary_2013.csv',\n",
       " '../../data/curated/features_of_interst/primary/primary_2014.csv',\n",
       " '../../data/curated/features_of_interst/primary/primary_2015.csv',\n",
       " '../../data/curated/features_of_interst/primary/primary_2016.csv',\n",
       " '../../data/curated/features_of_interst/primary/primary_2017.csv',\n",
       " '../../data/curated/features_of_interst/primary/primary_2018.csv',\n",
       " '../../data/curated/features_of_interst/primary/primary_2019.csv',\n",
       " '../../data/curated/features_of_interst/primary/primary_2020.csv',\n",
       " '../../data/curated/features_of_interst/primary/primary_2021.csv',\n",
       " '../../data/curated/features_of_interst/secondary/secondary_2013.csv',\n",
       " '../../data/curated/features_of_interst/secondary/secondary_2014.csv',\n",
       " '../../data/curated/features_of_interst/secondary/secondary_2015.csv',\n",
       " '../../data/curated/features_of_interst/secondary/secondary_2016.csv',\n",
       " '../../data/curated/features_of_interst/secondary/secondary_2017.csv',\n",
       " '../../data/curated/features_of_interst/secondary/secondary_2018.csv',\n",
       " '../../data/curated/features_of_interst/secondary/secondary_2019.csv',\n",
       " '../../data/curated/features_of_interst/secondary/secondary_2020.csv',\n",
       " '../../data/curated/features_of_interst/secondary/secondary_2021.csv',\n",
       " '../../data/curated/features_of_interst/train_station/station_2013.csv',\n",
       " '../../data/curated/features_of_interst/train_station/station_2014.csv',\n",
       " '../../data/curated/features_of_interst/train_station/station_2015.csv',\n",
       " '../../data/curated/features_of_interst/train_station/station_2016.csv',\n",
       " '../../data/curated/features_of_interst/train_station/station_2017.csv',\n",
       " '../../data/curated/features_of_interst/train_station/station_2018.csv',\n",
       " '../../data/curated/features_of_interst/train_station/station_2019.csv',\n",
       " '../../data/curated/features_of_interst/train_station/station_2020.csv',\n",
       " '../../data/curated/features_of_interst/train_station/station_2021.csv']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_path = '../../data/curated/features_of_interst/'\n",
    "\n",
    "child_path = ['park', 'primary', 'secondary', 'train_station']\n",
    "pathlst = []\n",
    "for child in child_path:\n",
    "    path = str(parent_path)+str(child)+\"/*.csv\"\n",
    "    pathlst.append(path)\n",
    "pathlst\n",
    "\n",
    "facility_all_lst = []\n",
    "for path in pathlst:\n",
    "    for fname in glob.glob(path):\n",
    "        facility_all_lst.append(fname)\n",
    "facility_all_lst = sorted(facility_all_lst)\n",
    "facility_all_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_sa2(data, sf, name):\n",
    "    data = data.dropna(subset=['longitude', 'latitude'])\n",
    "    #if \n",
    "    data['SA2_CODE'] = np.nan\n",
    "    data = data.astype({'longitude': 'float', 'latitude': 'float'})\n",
    "\n",
    "    print(\"Estimated processing size:\", len(sf) * len(data))\n",
    "\n",
    "    # Allocate SA2 code based on the coordinates from rent data\n",
    "    for index, row in data.iterrows():\n",
    "        for index_area, row_area in sf.iterrows():\n",
    "            geo = row_area[\"geometry\"]\n",
    "            fit = False         \n",
    "            fit = geo.contains(Point(row[\"longitude\"], row[\"latitude\"]))\n",
    "\n",
    "            if fit:\n",
    "                data.loc[index,'SA2_CODE'] = row_area[\"SA2_CODE21\"]\n",
    "                # print(row['id'], \":\", row_area[\"SA2_CODE21\"])\n",
    "                break\n",
    "\n",
    "    # Show data loss\n",
    "    len_data = len(data)\n",
    "    len_result = len(data.dropna(subset=['SA2_CODE']))\n",
    "    print(\"Original size:\", len_data, \"=> Result size:\", len_result)\n",
    "    print(\"Loss:\", len_data - len_result)\n",
    "\n",
    "    # Convert SA2 code as Interger\n",
    "    data = data.dropna(subset=['SA2_CODE'])\n",
    "    data['SA2_CODE'] = data[\"SA2_CODE\"].astype(int)\n",
    "\n",
    "    # Export as csv\n",
    "    data.to_csv(\"../../data/curated/features_of_interst/\"+name+\"_with_SA2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "park_2013\n",
      "Estimated processing size: 0\n",
      "Original size: 0 => Result size: 0\n",
      "Loss: 0\n",
      "park_2014\n",
      "Estimated processing size: 26100\n",
      "Original size: 50 => Result size: 50\n",
      "Loss: 0\n",
      "park_2015\n",
      "Estimated processing size: 5220\n",
      "Original size: 10 => Result size: 10\n",
      "Loss: 0\n",
      "park_2016\n",
      "Estimated processing size: 12528\n",
      "Original size: 24 => Result size: 24\n",
      "Loss: 0\n",
      "park_2017\n",
      "Estimated processing size: 34974\n",
      "Original size: 67 => Result size: 67\n",
      "Loss: 0\n",
      "park_2018\n",
      "Estimated processing size: 47502\n",
      "Original size: 91 => Result size: 91\n",
      "Loss: 0\n",
      "park_2019\n",
      "Estimated processing size: 65772\n",
      "Original size: 126 => Result size: 126\n",
      "Loss: 0\n",
      "park_2020\n",
      "Estimated processing size: 8874\n",
      "Original size: 17 => Result size: 17\n",
      "Loss: 0\n",
      "park_2021\n",
      "Estimated processing size: 7830\n",
      "Original size: 15 => Result size: 15\n",
      "Loss: 0\n",
      "primary_2013\n",
      "Estimated processing size: 0\n",
      "Original size: 0 => Result size: 0\n",
      "Loss: 0\n",
      "primary_2014\n",
      "Estimated processing size: 37062\n",
      "Original size: 71 => Result size: 0\n",
      "Loss: 71\n",
      "primary_2015\n",
      "Estimated processing size: 38628\n",
      "Original size: 74 => Result size: 0\n",
      "Loss: 74\n",
      "primary_2016\n",
      "Estimated processing size: 39672\n",
      "Original size: 76 => Result size: 0\n",
      "Loss: 76\n",
      "primary_2017\n",
      "Estimated processing size: 44370\n",
      "Original size: 85 => Result size: 0\n",
      "Loss: 85\n",
      "primary_2018\n",
      "Estimated processing size: 1566\n",
      "Original size: 3 => Result size: 0\n",
      "Loss: 3\n",
      "primary_2019\n",
      "Estimated processing size: 6264\n",
      "Original size: 12 => Result size: 0\n",
      "Loss: 12\n",
      "primary_2020\n",
      "Estimated processing size: 12006\n",
      "Original size: 23 => Result size: 0\n",
      "Loss: 23\n",
      "primary_2021\n",
      "Estimated processing size: 22446\n",
      "Original size: 43 => Result size: 0\n",
      "Loss: 43\n",
      "secondary_2013\n",
      "Estimated processing size: 0\n",
      "Original size: 0 => Result size: 0\n",
      "Loss: 0\n",
      "secondary_2014\n",
      "Estimated processing size: 13050\n",
      "Original size: 25 => Result size: 25\n",
      "Loss: 0\n",
      "secondary_2015\n",
      "Estimated processing size: 15138\n",
      "Original size: 29 => Result size: 29\n",
      "Loss: 0\n",
      "secondary_2016\n",
      "Estimated processing size: 15660\n",
      "Original size: 30 => Result size: 30\n",
      "Loss: 0\n",
      "secondary_2017\n",
      "Estimated processing size: 16704\n",
      "Original size: 32 => Result size: 32\n",
      "Loss: 0\n",
      "secondary_2018\n",
      "Estimated processing size: 19314\n",
      "Original size: 37 => Result size: 37\n",
      "Loss: 0\n",
      "secondary_2019\n",
      "Estimated processing size: 22968\n",
      "Original size: 44 => Result size: 44\n",
      "Loss: 0\n",
      "secondary_2020\n",
      "Estimated processing size: 29754\n",
      "Original size: 57 => Result size: 57\n",
      "Loss: 0\n",
      "secondary_2021\n",
      "Estimated processing size: 33408\n",
      "Original size: 64 => Result size: 64\n",
      "Loss: 0\n",
      "station_2013\n",
      "Estimated processing size: 0\n",
      "Original size: 0 => Result size: 0\n",
      "Loss: 0\n",
      "station_2014\n",
      "Estimated processing size: 1044\n",
      "Original size: 2 => Result size: 2\n",
      "Loss: 0\n",
      "station_2015\n",
      "Estimated processing size: 2088\n",
      "Original size: 4 => Result size: 4\n",
      "Loss: 0\n",
      "station_2016\n",
      "Estimated processing size: 2610\n",
      "Original size: 5 => Result size: 5\n",
      "Loss: 0\n",
      "station_2017\n",
      "Estimated processing size: 3132\n",
      "Original size: 6 => Result size: 6\n",
      "Loss: 0\n",
      "station_2018\n",
      "Estimated processing size: 3132\n",
      "Original size: 6 => Result size: 6\n",
      "Loss: 0\n",
      "station_2019\n",
      "Estimated processing size: 7308\n",
      "Original size: 14 => Result size: 14\n",
      "Loss: 0\n",
      "station_2020\n",
      "Estimated processing size: 7830\n",
      "Original size: 15 => Result size: 15\n",
      "Loss: 0\n",
      "station_2021\n",
      "Estimated processing size: 8352\n",
      "Original size: 16 => Result size: 16\n",
      "Loss: 0\n"
     ]
    }
   ],
   "source": [
    "for path in facility_all_lst:\n",
    "    \n",
    "    regex = r'\\w+_\\d+'\n",
    "    name = re.findall(regex, path)[0]\n",
    "    print(name)\n",
    "    # Read Rent Data\n",
    "    data = pd.read_csv(path)\n",
    "    #print(data.longitude.sum())\n",
    "    add_sa2(data, sf, name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_sa2_path1 = '../../data/curated/features_of_interst/*.csv'\n",
    "file_lst2 = []\n",
    "for fname in glob.glob(added_sa2_path1):\n",
    "    file_lst2.append(fname)\n",
    "facility_all_with_sa2 = pd.concat([pd.read_csv(f) for f in file_lst2 ])\n",
    "facility_all_with_sa2.to_csv( \"../../data/curated/features_of_interst/facility_all_with_sa2.csv\", index=False, encoding='utf-8-sig') \n",
    "\n",
    "for fname in file_lst2:\n",
    "    os.remove(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import shutil\n",
    "for path in pathlst:\n",
    "shutil.rmtree(path)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import folium\n",
    "# import numpy as np\n",
    "# \n",
    "# # make geometry as JSON type\n",
    "# geoJSON = sf['geometry'].to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Map whole SA2 area\n",
    "# _map = folium.Map(location=[-37, 144], tiles=\"Stamen Terrain\", zoom_start=10)\n",
    "# \n",
    "# _map.add_child(folium.Choropleth(\n",
    "#     geo_data=geoJSON,\n",
    "#     name='SA2 Area',\n",
    "# ))\n",
    "# \n",
    "# _map.save('../../plots/SA2_Map.html')\n",
    "# _map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Point rent data in the map\n",
    "# \n",
    "# data_s = data.dropna(subset=['latitude', 'longitude'])\n",
    "# \n",
    "# id_data = data_s['id']\n",
    "# latitude_data = data_s['latitude']\n",
    "# longitude_data = data_s['longitude']\n",
    "# \n",
    "# # plot points out of geolocation\n",
    "# for id, lati, long in zip(id_data, latitude_data, longitude_data):\n",
    "#     _map.add_child(\n",
    "#         folium.Marker(location=[lati, long], popup=str(id))\n",
    "#     )\n",
    "# \n",
    "# _map.save('../../plots/rentalData_in_SA2Location.html')\n",
    "# _map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c86cb5e9148e7c4bccb8072cef861029eee896e08e86645269b0451f021733e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
