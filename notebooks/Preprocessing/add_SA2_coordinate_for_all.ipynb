{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "import os\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_sa2(data, sf, name):\n",
    "    data = data.dropna(subset=['longitude', 'latitude'])\n",
    "    #if \n",
    "    data['SA2_CODE'] = np.nan\n",
    "    data = data.astype({'longitude': 'float', 'latitude': 'float'})\n",
    "\n",
    "    print(\"Estimated processing size:\", len(sf) * len(data))\n",
    "\n",
    "    # Allocate SA2 code based on the coordinates from rent data\n",
    "    for index, row in data.iterrows():\n",
    "        for index_area, row_area in sf.iterrows():\n",
    "            geo = row_area[\"geometry\"]\n",
    "            fit = False         \n",
    "            fit = geo.contains(Point(row[\"longitude\"], row[\"latitude\"]))\n",
    "\n",
    "            if fit:\n",
    "                data.loc[index,'SA2_CODE'] = row_area[\"SA2_CODE21\"]\n",
    "                # print(row['id'], \":\", row_area[\"SA2_CODE21\"])\n",
    "                break\n",
    "\n",
    "    # Show data loss\n",
    "    len_data = len(data)\n",
    "    len_result = len(data.dropna(subset=['SA2_CODE']))\n",
    "    print(\"Original size:\", len_data, \"=> Result size:\", len_result)\n",
    "    print(\"Loss:\", len_data - len_result)\n",
    "\n",
    "    # Convert SA2 code as Interger\n",
    "    data = data.dropna(subset=['SA2_CODE'])\n",
    "    data['SA2_CODE'] = data[\"SA2_CODE\"].astype(int)\n",
    "\n",
    "    # Export as csv\n",
    "    data.to_csv(\"../../data/curated/property_all_with_SA2/\"+name+\"_property_with_SA2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read SA2-Geolocation data (shape file)\n",
    "# https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files\n",
    "sf = gpd.read_file(\"../../data/raw/Geo/SA2_2021_AUST_SHP_GDA2020/SA2_2021_AUST_GDA2020.shp\")\n",
    "\n",
    "# Slice the geolocation for Victoria\n",
    "COL_SF = [\"SA2_CODE21\", \"geometry\"]\n",
    "sf = sf.loc[sf[\"STE_CODE21\"] == '2']\n",
    "sf['geometry'] = sf['geometry'].to_crs(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\")\n",
    "sf = sf[COL_SF]\n",
    "sf = sf.dropna(subset=['geometry'])\n",
    "sf[\"SA2_CODE21\"] = sf[\"SA2_CODE21\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this part should be run after running History_prepro.ipynb, files and directory will be deleted after for space saving\n",
    "path = \"../../data/curated/property_all_no_outlier/*.csv\"\n",
    "property_all_lst = []\n",
    "for fname in glob.glob(path):\n",
    "    property_all_lst.append(fname)\n",
    "property_all_lst = sorted(property_all_lst)\n",
    "property_all_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006\n",
      "Estimated processing size: 267264\n",
      "Original size: 512 => Result size: 512\n",
      "Loss: 0\n",
      "2007\n",
      "Estimated processing size: 2418426\n",
      "Original size: 4633 => Result size: 4632\n",
      "Loss: 1\n",
      "2008\n",
      "Estimated processing size: 2816712\n",
      "Original size: 5396 => Result size: 5376\n",
      "Loss: 20\n",
      "2009\n",
      "Estimated processing size: 2650716\n",
      "Original size: 5078 => Result size: 5058\n",
      "Loss: 20\n",
      "2010\n",
      "Estimated processing size: 3201426\n",
      "Original size: 6133 => Result size: 6081\n",
      "Loss: 52\n",
      "2011\n",
      "Estimated processing size: 4530960\n",
      "Original size: 8680 => Result size: 8558\n",
      "Loss: 122\n",
      "2012\n",
      "Estimated processing size: 5698674\n",
      "Original size: 10917 => Result size: 10830\n",
      "Loss: 87\n",
      "2013\n",
      "Estimated processing size: 6039018\n",
      "Original size: 11569 => Result size: 11466\n",
      "Loss: 103\n",
      "2014\n",
      "Estimated processing size: 6544836\n",
      "Original size: 12538 => Result size: 12489\n",
      "Loss: 49\n",
      "2015\n",
      "Estimated processing size: 6748416\n",
      "Original size: 12928 => Result size: 12782\n",
      "Loss: 146\n",
      "2016\n",
      "Estimated processing size: 7968852\n",
      "Original size: 15266 => Result size: 15119\n",
      "Loss: 147\n",
      "2017\n",
      "Estimated processing size: 9046782\n",
      "Original size: 17331 => Result size: 17143\n",
      "Loss: 188\n",
      "2018\n",
      "Estimated processing size: 10375794\n",
      "Original size: 19877 => Result size: 19604\n",
      "Loss: 273\n",
      "2019\n",
      "Estimated processing size: 11406222\n",
      "Original size: 21851 => Result size: 21554\n",
      "Loss: 297\n",
      "2020\n",
      "Estimated processing size: 11477214\n",
      "Original size: 21987 => Result size: 21638\n",
      "Loss: 349\n",
      "2021\n",
      "Estimated processing size: 14112270\n",
      "Original size: 27035 => Result size: 26595\n",
      "Loss: 440\n",
      "2022\n",
      "Estimated processing size: 38439558\n",
      "Original size: 73639 => Result size: 73107\n",
      "Loss: 532\n"
     ]
    }
   ],
   "source": [
    "for path in property_all_lst:\n",
    "    \n",
    "    regex = r'\\d+\\w\\d+'\n",
    "    year = re.findall(regex, path)[0]\n",
    "    print(year)\n",
    "    # Read Rent Data\n",
    "    data = pd.read_csv(path)\n",
    "    #print(data.longitude.sum())\n",
    "    add_sa2(data, sf, year)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "no_outlier_path = '../../data/curated/property_all_no_outlier'\n",
    "shutil.rmtree(no_outlier_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding SA2 For Facilities (School, Park, Train Station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../data/curated/features_of_interst/park/park_2013.csv',\n",
       " '../../data/curated/features_of_interst/park/park_2014.csv',\n",
       " '../../data/curated/features_of_interst/park/park_2015.csv',\n",
       " '../../data/curated/features_of_interst/park/park_2016.csv',\n",
       " '../../data/curated/features_of_interst/park/park_2017.csv',\n",
       " '../../data/curated/features_of_interst/park/park_2018.csv',\n",
       " '../../data/curated/features_of_interst/park/park_2019.csv',\n",
       " '../../data/curated/features_of_interst/park/park_2020.csv',\n",
       " '../../data/curated/features_of_interst/park/park_2021.csv',\n",
       " '../../data/curated/features_of_interst/primary/primary_2013.csv',\n",
       " '../../data/curated/features_of_interst/primary/primary_2014.csv',\n",
       " '../../data/curated/features_of_interst/primary/primary_2015.csv',\n",
       " '../../data/curated/features_of_interst/primary/primary_2016.csv',\n",
       " '../../data/curated/features_of_interst/primary/primary_2017.csv',\n",
       " '../../data/curated/features_of_interst/primary/primary_2018.csv',\n",
       " '../../data/curated/features_of_interst/primary/primary_2019.csv',\n",
       " '../../data/curated/features_of_interst/primary/primary_2020.csv',\n",
       " '../../data/curated/features_of_interst/primary/primary_2021.csv',\n",
       " '../../data/curated/features_of_interst/secondary/secondary_2013.csv',\n",
       " '../../data/curated/features_of_interst/secondary/secondary_2014.csv',\n",
       " '../../data/curated/features_of_interst/secondary/secondary_2015.csv',\n",
       " '../../data/curated/features_of_interst/secondary/secondary_2016.csv',\n",
       " '../../data/curated/features_of_interst/secondary/secondary_2017.csv',\n",
       " '../../data/curated/features_of_interst/secondary/secondary_2018.csv',\n",
       " '../../data/curated/features_of_interst/secondary/secondary_2019.csv',\n",
       " '../../data/curated/features_of_interst/secondary/secondary_2020.csv',\n",
       " '../../data/curated/features_of_interst/secondary/secondary_2021.csv',\n",
       " '../../data/curated/features_of_interst/train_station/station_2013.csv',\n",
       " '../../data/curated/features_of_interst/train_station/station_2014.csv',\n",
       " '../../data/curated/features_of_interst/train_station/station_2015.csv',\n",
       " '../../data/curated/features_of_interst/train_station/station_2016.csv',\n",
       " '../../data/curated/features_of_interst/train_station/station_2017.csv',\n",
       " '../../data/curated/features_of_interst/train_station/station_2018.csv',\n",
       " '../../data/curated/features_of_interst/train_station/station_2019.csv',\n",
       " '../../data/curated/features_of_interst/train_station/station_2020.csv',\n",
       " '../../data/curated/features_of_interst/train_station/station_2021.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_path = '../../data/curated/features_of_interst/'\n",
    "\n",
    "child_path = ['park', 'primary', 'secondary', 'train_station']\n",
    "pathlst = []\n",
    "for child in child_path:\n",
    "    path = str(parent_path)+str(child)+\"/*.csv\"\n",
    "    pathlst.append(path)\n",
    "pathlst\n",
    "\n",
    "facility_all_lst = []\n",
    "for path in pathlst:\n",
    "    for fname in glob.glob(path):\n",
    "        facility_all_lst.append(fname)\n",
    "facility_all_lst = sorted(facility_all_lst)\n",
    "facility_all_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_sa2(data, sf, name):\n",
    "    data = data.dropna(subset=['longitude', 'latitude'])\n",
    "    #if \n",
    "    data['SA2_CODE'] = np.nan\n",
    "    data = data.astype({'longitude': 'float', 'latitude': 'float'})\n",
    "\n",
    "    print(\"Estimated processing size:\", len(sf) * len(data))\n",
    "\n",
    "    # Allocate SA2 code based on the coordinates from rent data\n",
    "    for index, row in data.iterrows():\n",
    "        for index_area, row_area in sf.iterrows():\n",
    "            geo = row_area[\"geometry\"]\n",
    "            fit = False         \n",
    "            fit = geo.contains(Point(row[\"longitude\"], row[\"latitude\"]))\n",
    "\n",
    "            if fit:\n",
    "                data.loc[index,'SA2_CODE'] = row_area[\"SA2_CODE21\"]\n",
    "                # print(row['id'], \":\", row_area[\"SA2_CODE21\"])\n",
    "                break\n",
    "\n",
    "    # Show data loss\n",
    "    len_data = len(data)\n",
    "    len_result = len(data.dropna(subset=['SA2_CODE']))\n",
    "    print(\"Original size:\", len_data, \"=> Result size:\", len_result)\n",
    "    print(\"Loss:\", len_data - len_result)\n",
    "\n",
    "    # Convert SA2 code as Interger\n",
    "    data = data.dropna(subset=['SA2_CODE'])\n",
    "    data['SA2_CODE'] = data[\"SA2_CODE\"].astype(int)\n",
    "\n",
    "    # Export as csv\n",
    "    data.to_csv(\"../../data/curated/features_of_interst/\"+name+\"_with_SA2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primary_2013\n",
      "Estimated processing size: 998586\n",
      "Original size: 1913 => Result size: 1913\n",
      "Loss: 0\n",
      "primary_2014\n",
      "Estimated processing size: 1299258\n",
      "Original size: 2489 => Result size: 2489\n",
      "Loss: 0\n",
      "primary_2015\n",
      "Estimated processing size: 1300824\n",
      "Original size: 2492 => Result size: 2492\n",
      "Loss: 0\n",
      "primary_2016\n",
      "Estimated processing size: 1301868\n",
      "Original size: 2494 => Result size: 2494\n",
      "Loss: 0\n",
      "primary_2017\n",
      "Estimated processing size: 1306566\n",
      "Original size: 2503 => Result size: 2503\n",
      "Loss: 0\n",
      "primary_2018\n",
      "Estimated processing size: 1308654\n",
      "Original size: 2507 => Result size: 2507\n",
      "Loss: 0\n",
      "primary_2019\n",
      "Estimated processing size: 1313352\n",
      "Original size: 2516 => Result size: 2516\n",
      "Loss: 0\n",
      "primary_2020\n",
      "Estimated processing size: 1319094\n",
      "Original size: 2527 => Result size: 2527\n",
      "Loss: 0\n",
      "primary_2021\n",
      "Estimated processing size: 1329534\n",
      "Original size: 2547 => Result size: 2547\n",
      "Loss: 0\n",
      "secondary_2013\n",
      "Estimated processing size: 447354\n",
      "Original size: 857 => Result size: 857\n",
      "Loss: 0\n",
      "secondary_2014\n",
      "Estimated processing size: 693216\n",
      "Original size: 1328 => Result size: 1328\n",
      "Loss: 0\n",
      "secondary_2015\n",
      "Estimated processing size: 695304\n",
      "Original size: 1332 => Result size: 1332\n",
      "Loss: 0\n",
      "secondary_2016\n",
      "Estimated processing size: 695826\n",
      "Original size: 1333 => Result size: 1333\n",
      "Loss: 0\n",
      "secondary_2017\n",
      "Estimated processing size: 696870\n",
      "Original size: 1335 => Result size: 1335\n",
      "Loss: 0\n",
      "secondary_2018\n",
      "Estimated processing size: 699480\n",
      "Original size: 1340 => Result size: 1340\n",
      "Loss: 0\n",
      "secondary_2019\n",
      "Estimated processing size: 703134\n",
      "Original size: 1347 => Result size: 1347\n",
      "Loss: 0\n",
      "secondary_2020\n",
      "Estimated processing size: 709920\n",
      "Original size: 1360 => Result size: 1360\n",
      "Loss: 0\n",
      "secondary_2021\n",
      "Estimated processing size: 713574\n",
      "Original size: 1367 => Result size: 1367\n",
      "Loss: 0\n",
      "station_2013\n",
      "Estimated processing size: 451008\n",
      "Original size: 864 => Result size: 864\n",
      "Loss: 0\n",
      "station_2014\n",
      "Estimated processing size: 452052\n",
      "Original size: 866 => Result size: 866\n",
      "Loss: 0\n",
      "station_2015\n",
      "Estimated processing size: 453096\n",
      "Original size: 868 => Result size: 868\n",
      "Loss: 0\n",
      "station_2016\n",
      "Estimated processing size: 453618\n",
      "Original size: 869 => Result size: 869\n",
      "Loss: 0\n",
      "station_2017\n",
      "Estimated processing size: 454140\n",
      "Original size: 870 => Result size: 870\n",
      "Loss: 0\n",
      "station_2018\n",
      "Estimated processing size: 454140\n",
      "Original size: 870 => Result size: 870\n",
      "Loss: 0\n",
      "station_2019\n",
      "Estimated processing size: 458316\n",
      "Original size: 878 => Result size: 878\n",
      "Loss: 0\n",
      "station_2020\n",
      "Estimated processing size: 458838\n",
      "Original size: 879 => Result size: 879\n",
      "Loss: 0\n",
      "station_2021\n",
      "Estimated processing size: 459360\n",
      "Original size: 880 => Result size: 880\n",
      "Loss: 0\n"
     ]
    }
   ],
   "source": [
    "for path in facility_all_lst:\n",
    "    regex = r'\\w+_\\d+'\n",
    "    regex2 = r'\\d+'\n",
    "    name = re.findall(regex, path)[0]\n",
    "    year = re.findall(regex2, path)[0]\n",
    "    print(name)\n",
    "    # Read Rent Data\n",
    "    data = pd.read_csv(path)\n",
    "    data = data.rename(columns={'Longitude': 'latitude', 'Latitude':'longitude'})\n",
    "    data['year'] = year\n",
    "    data['place_type'] = name[:-5]\n",
    "    #print(data.longitude.sum())\n",
    "    add_sa2(data, sf, name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_sa2_path1 = '../../data/curated/features_of_interst/*.csv'\n",
    "file_lst2 = []\n",
    "for fname in glob.glob(added_sa2_path1):\n",
    "    file_lst2.append(fname)\n",
    "facility_all_with_sa2 = pd.concat([pd.read_csv(f) for f in file_lst2 ])\n",
    "facility_all_with_sa2.to_csv( \"../../data/curated/features_of_interst/place_all_with_sa2.csv\", index=False, encoding='utf-8-sig') \n",
    "\n",
    "for fname in file_lst2:\n",
    "    os.remove(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import shutil\n",
    "for path in pathlst:\n",
    "shutil.rmtree(path)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import folium\n",
    "# import numpy as np\n",
    "# \n",
    "# # make geometry as JSON type\n",
    "# geoJSON = sf['geometry'].to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Map whole SA2 area\n",
    "# _map = folium.Map(location=[-37, 144], tiles=\"Stamen Terrain\", zoom_start=10)\n",
    "# \n",
    "# _map.add_child(folium.Choropleth(\n",
    "#     geo_data=geoJSON,\n",
    "#     name='SA2 Area',\n",
    "# ))\n",
    "# \n",
    "# _map.save('../../plots/SA2_Map.html')\n",
    "# _map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Point rent data in the map\n",
    "# \n",
    "# data_s = data.dropna(subset=['latitude', 'longitude'])\n",
    "# \n",
    "# id_data = data_s['id']\n",
    "# latitude_data = data_s['latitude']\n",
    "# longitude_data = data_s['longitude']\n",
    "# \n",
    "# # plot points out of geolocation\n",
    "# for id, lati, long in zip(id_data, latitude_data, longitude_data):\n",
    "#     _map.add_child(\n",
    "#         folium.Marker(location=[lati, long], popup=str(id))\n",
    "#     )\n",
    "# \n",
    "# _map.save('../../plots/rentalData_in_SA2Location.html')\n",
    "# _map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c86cb5e9148e7c4bccb8072cef861029eee896e08e86645269b0451f021733e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
